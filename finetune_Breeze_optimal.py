#!/usr/bin/env python3
"""
Breeze-ASR-25 æœ€ä½³åŒ–å¾®èª¿ç‰ˆæœ¬
çµåˆ train.py çš„è¨˜æ†¶é«”æ•ˆç‡å’Œ finetune_Breeze_whisper.py çš„åŠŸèƒ½æ€§
é‡å° RTX 3060 Ti 8GB å„ªåŒ–ï¼Œç¢ºä¿ç©©å®šé«˜æ•ˆçš„è¨“ç·´
"""

import multiprocessing
import os
from dataclasses import dataclass
from functools import partial
from pathlib import Path
from typing import Dict, List, Optional, Union

import jiwer
import librosa
import numpy as np
import pandas as pd
import torch
from datasets import Audio, Dataset, DatasetDict
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperTokenizer,
)

# ==============================================================================
# å…¨åŸŸé…ç½®å’Œè®Šæ•¸ (ç¢ºä¿å¤šé€²ç¨‹ç›¸å®¹æ€§)
# ==============================================================================

MODEL_ID = "MediaTek-Research/Breeze-ASR-25"
DATASET_DIR = Path("./")
TRAIN_CSV = DATASET_DIR / "metadata_train.csv"
TEST_CSV = DATASET_DIR / "metadata_test.csv"
OUTPUT_DIR = "./breeze-asr-25-optimal"

# å…¨åŸŸ processor è®Šæ•¸
_processor = None


def init_processor(proc):
    """åˆå§‹åŒ–å…¨åŸŸ processor"""
    global _processor
    _processor = proc


# ==============================================================================
# è³‡æ–™è™•ç†å‡½æ•¸ (å…¨åŸŸå®šç¾©ä»¥æ”¯æ´å¤šé€²ç¨‹)
# ==============================================================================


def prepare_dataset_optimal(batch, processor=None):
    """
    æœ€ä½³åŒ–çš„å³æ™‚è³‡æ–™è½‰æ›å‡½æ•¸
    çµåˆ train.py çš„è¨˜æ†¶é«”æ•ˆç‡å’Œ Breeze çš„åŠŸèƒ½æ€§
    """
    import librosa
    import numpy as np

    # ä½¿ç”¨å‚³å…¥çš„ processor æˆ–å…¨åŸŸ processor
    current_processor = processor if processor is not None else _processor

    if current_processor is None:
        print("éŒ¯èª¤ï¼šç„¡æ³•å–å¾— processor")
        return batch

    # è™•ç†éŸ³è¨Šè³‡æ–™
    audio_list = []
    for audio_path in batch["file"]:
        try:
            # è¼‰å…¥éŸ³è¨Šä¸¦ç¢ºä¿ 16kHz æ¡æ¨£ç‡
            audio_array, sampling_rate = librosa.load(audio_path, sr=16000)
            audio_list.append({"array": audio_array, "sampling_rate": sampling_rate})
        except Exception as e:
            print(f"è­¦å‘Šï¼šè¼‰å…¥éŸ³è¨Šå¤±æ•— {audio_path}: {e}")
            # ä½¿ç”¨éœéŸ³æ›¿ä»£
            audio_list.append({"array": np.zeros(16000), "sampling_rate": 16000})

    # è™•ç†éŸ³è¨Šç‰¹å¾µ - ä½¿ç”¨ Whisper çš„æ¨™æº–è™•ç†æ–¹å¼
    try:
        batch["input_features"] = current_processor.feature_extractor(
            [x["array"] for x in audio_list],
            sampling_rate=16000,
            return_tensors="np",  # è¿”å› numpy æ ¼å¼é¿å… GPU è¨˜æ†¶é«”ç´¯ç©
        ).input_features
    except Exception as e:
        print(f"éŸ³è¨Šç‰¹å¾µæå–å¤±æ•—: {e}")
        # å‰µå»ºç©ºçš„ç‰¹å¾µä½œç‚ºå‚™ç”¨
        batch["input_features"] = np.zeros((len(audio_list), 80, 3000))

    # è™•ç†æ–‡å­—æ¨™ç±¤ - ä½¿ç”¨ä¸­æ–‡æ„è­¯æ¬„ä½
    try:
        texts = [str(text).strip() for text in batch["ä¸­æ–‡æ„è­¯"]]
        # éæ¿¾ç©ºæ–‡å­—
        texts = [text if text else "ç„¡å…§å®¹" for text in texts]

        batch["labels"] = current_processor.tokenizer(
            texts,
            max_length=448,
            truncation=True,
            return_tensors="np",  # è¿”å› numpy æ ¼å¼
        ).input_ids
    except Exception as e:
        print(f"æ–‡å­—æ¨™ç±¤è™•ç†å¤±æ•—: {e}")
        # å‰µå»ºç©ºæ¨™ç±¤ä½œç‚ºå‚™ç”¨
        batch["labels"] = np.zeros((len(batch["ä¸­æ–‡æ„è­¯"]), 1))

    return batch


@dataclass
class DataCollatorSpeechSeq2SeqOptimal:
    """
    æœ€ä½³åŒ–çš„ Data Collator
    çµåˆè¨˜æ†¶é«”æ•ˆç‡å’ŒåŠŸèƒ½å®Œæ•´æ€§
    """

    processor: WhisperProcessor
    decoder_start_token_id: Optional[int] = None

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # è™•ç† input_features
        input_features = [
            {
                "input_features": torch.tensor(
                    feature["input_features"], dtype=torch.float32
                )
            }
            for feature in features
        ]

        # ä½¿ç”¨ processor é€²è¡Œå¡«å……
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )

        # è™•ç†æ¨™ç±¤
        label_features = [
            {"input_ids": torch.tensor(feature["labels"], dtype=torch.long)}
            for feature in features
        ]

        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")

        # è™•ç†æ¨™ç±¤é®ç½©
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )

        # ç§»é™¤ BOS tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
            labels = labels[:, 1:]

        batch["labels"] = labels
        return batch


def compute_metrics_optimal(pred, processor=None):
    """
    æœ€ä½³åŒ–çš„è©•ä¼°æŒ‡æ¨™è¨ˆç®—
    ä½¿ç”¨ç™¾åˆ†æ¯” WER ä»¥ä¾¿æ–¼ç†è§£
    """
    # ä½¿ç”¨å‚³å…¥çš„ processor æˆ–å…¨åŸŸ processor
    current_processor = processor if processor is not None else _processor

    if current_processor is None:
        print("éŒ¯èª¤ï¼šç„¡æ³•å–å¾— processor é€²è¡Œè©•ä¼°")
        return {"wer": 100.0}

    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # å°‡ -100 æ›¿æ›ç‚º pad token
    label_ids[label_ids == -100] = current_processor.tokenizer.pad_token_id

    # è§£ç¢¼é æ¸¬å’Œæ¨™ç±¤
    pred_str = current_processor.tokenizer.batch_decode(
        pred_ids, skip_special_tokens=True
    )
    label_str = current_processor.tokenizer.batch_decode(
        label_ids, skip_special_tokens=True
    )

    # è¨ˆç®— WERï¼ˆç™¾åˆ†æ¯”ï¼‰
    wer = 100 * jiwer.wer(label_str, pred_str)

    # è¨ˆç®—é¡å¤–æŒ‡æ¨™
    try:
        # è¨ˆç®—å­—ç¬¦éŒ¯èª¤ç‡
        cer = 100 * jiwer.cer(label_str, pred_str)
        return {"wer": wer, "cer": cer}
    except:
        return {"wer": wer}


# ==============================================================================
# ä¸»è¦è¨“ç·´å‡½æ•¸
# ==============================================================================


def main():
    """
    æœ€ä½³åŒ–çš„ä¸»è¦è¨“ç·´æµç¨‹
    çµåˆå…©å€‹ç‰ˆæœ¬çš„å„ªå‹¢
    """
    global _processor

    # Windows å¤šé€²ç¨‹æ”¯æ´
    if __name__ == "__main__":
        multiprocessing.freeze_support()

    print("ğŸš€ Breeze-ASR-25 æœ€ä½³åŒ–å¾®èª¿é–‹å§‹")
    print("=" * 60)

    # è¨˜æ†¶é«”å„ªåŒ–è¨­å®š
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

    # æ¸…ç† GPU è¨˜æ†¶é«”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(
            f"GPU è¨˜æ†¶é«”å·²æ¸…ç†ï¼Œå¯ç”¨è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        )

    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
    if not TRAIN_CSV.exists() or not TEST_CSV.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ")
        print(f"   è¨“ç·´æª”æ¡ˆ: {TRAIN_CSV}")
        print(f"   æ¸¬è©¦æª”æ¡ˆ: {TEST_CSV}")
        return

    # è¼‰å…¥è³‡æ–™
    print("\nğŸ“Š è¼‰å…¥è³‡æ–™é›†...")
    train_df = pd.read_csv(TRAIN_CSV)
    test_df = pd.read_csv(TEST_CSV)

    print(f"   åŸå§‹è¨“ç·´è³‡æ–™: {len(train_df)} ç­†")
    print(f"   åŸå§‹æ¸¬è©¦è³‡æ–™: {len(test_df)} ç­†")

    # è³‡æ–™å“è³ªæª¢æŸ¥
    print("\nğŸ” è³‡æ–™å“è³ªæª¢æŸ¥...")

    # æª¢æŸ¥å¿…è¦æ¬„ä½
    required_columns = ["file", "ä¸­æ–‡æ„è­¯"]
    for col in required_columns:
        if col not in train_df.columns:
            print(f"âŒ éŒ¯èª¤ï¼šè¨“ç·´è³‡æ–™ç¼ºå°‘æ¬„ä½ '{col}'")
            return

    # ç§»é™¤ç©ºå€¼
    train_df = train_df.dropna(subset=required_columns)
    test_df = test_df.dropna(subset=required_columns)

    print(f"   æ¸…ç†å¾Œè¨“ç·´è³‡æ–™: {len(train_df)} ç­†")
    print(f"   æ¸…ç†å¾Œæ¸¬è©¦è³‡æ–™: {len(test_df)} ç­†")

    # è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨
    print("\nğŸ¤– è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨...")
    try:
        processor = WhisperProcessor.from_pretrained(MODEL_ID)
        model = WhisperForConditionalGeneration.from_pretrained(
            MODEL_ID,
            torch_dtype=torch.float32,  # ä½¿ç”¨ FP32 é¿å…æ¢¯åº¦å•é¡Œ
            low_cpu_mem_usage=True,
        )

        # æ¨¡å‹é…ç½®å„ªåŒ–
        model.config.use_cache = False  # é—œé–‰å¿«å–ç¯€çœè¨˜æ†¶é«”
        model.config.forced_decoder_ids = None
        model.config.suppress_tokens = []

        print("   âœ… æ¨¡å‹å’Œè™•ç†å™¨è¼‰å…¥æˆåŠŸ")

        # åˆå§‹åŒ–å…¨åŸŸ processor
        init_processor(processor)

    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return

    # å‰µå»ºè³‡æ–™é›† - ä½¿ç”¨å³æ™‚è½‰æ›ç­–ç•¥
    print("\nğŸ“‹ è¨­å®šè³‡æ–™é›†å’Œå³æ™‚è½‰æ›...")
    try:
        # åˆä½µè³‡æ–™ä¸¦é‡æ–°åˆ†å‰²ä»¥ç¢ºä¿åˆ†ä½ˆå‡å‹»
        full_df = pd.concat([train_df, test_df], ignore_index=True)
        full_dataset = Dataset.from_pandas(full_df)

        # é‡æ–°åˆ†å‰²è³‡æ–™
        dataset_split = full_dataset.train_test_split(test_size=0.2, seed=42)

        # è¨­å®šå³æ™‚è½‰æ›å‡½æ•¸ï¼Œå°‡ processor ä½œç‚ºåƒæ•¸å‚³é
        def prepare_dataset_with_processor(batch):
            return prepare_dataset_optimal(batch, processor)

        # ä½¿ç”¨ with_transform é€²è¡Œå³æ™‚è½‰æ›
        vectorized_datasets = dataset_split.with_transform(
            prepare_dataset_with_processor
        )

        train_dataset = vectorized_datasets["train"]
        test_dataset = vectorized_datasets["test"]

        print(f"   âœ… å³æ™‚è½‰æ›è¨­å®šå®Œæˆ")
        print(f"   æœ€çµ‚è¨“ç·´è³‡æ–™: {len(train_dataset)} ç­†")
        print(f"   æœ€çµ‚æ¸¬è©¦è³‡æ–™: {len(test_dataset)} ç­†")

    except Exception as e:
        print(f"âŒ è³‡æ–™é›†è¨­å®šå¤±æ•—: {e}")
        return

    # å‰µå»º Data Collator
    data_collator = DataCollatorSpeechSeq2SeqOptimal(
        processor=processor,
        decoder_start_token_id=model.config.decoder_start_token_id,
    )

    # æœ€ä½³åŒ–è¨“ç·´åƒæ•¸
    print("\nâš™ï¸  é…ç½®è¨“ç·´åƒæ•¸...")
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        # è¨˜æ†¶é«”å„ªåŒ–çš„æ‰¹æ¬¡è¨­å®š
        per_device_train_batch_size=4,  # é€²ä¸€æ­¥é™ä½ä»¥ç¢ºä¿ç©©å®š
        per_device_eval_batch_size=4,  # è©•ä¼°ä¹Ÿä½¿ç”¨å°æ‰¹æ¬¡
        gradient_accumulation_steps=4,  # å¢åŠ ç´¯ç©æ­¥æ•¸ (æœ‰æ•ˆæ‰¹æ¬¡ = 2*8 = 16)
        # å­¸ç¿’ç‡å’Œèª¿åº¦
        learning_rate=1e-5,  # ä¿å®ˆçš„å­¸ç¿’ç‡
        warmup_steps=500,  # é©åº¦çš„é ç†±
        max_steps=5000,  # é™åˆ¶ç¸½æ­¥æ•¸
        # è©•ä¼°å’Œç”Ÿæˆè¨­å®š
        eval_strategy="steps",
        eval_steps=500,  # é »ç¹è©•ä¼°ä»¥ç›£æ§é€²åº¦
        predict_with_generate=True,  # å•Ÿç”¨ç”Ÿæˆæ¨¡å¼è©•ä¼°
        generation_max_length=448,  # é™åˆ¶ç”Ÿæˆé•·åº¦
        # ä¿å­˜ç­–ç•¥
        save_steps=500,  # é »ç¹ä¿å­˜
        save_total_limit=3,  # ä¿ç•™æœ€è¿‘ 3 å€‹æª¢æŸ¥é»
        load_best_model_at_end=True,  # è¼‰å…¥æœ€ä½³æ¨¡å‹
        metric_for_best_model="wer",  # ä½¿ç”¨ WER ä½œç‚ºè©•ä¼°æŒ‡æ¨™
        greater_is_better=False,  # WER è¶Šä½è¶Šå¥½
        # è¨˜æ†¶é«”å’Œæ•ˆèƒ½å„ªåŒ–
        fp16=True,  # å•Ÿç”¨ FP16 ç¯€çœè¨˜æ†¶é«”
        gradient_checkpointing=False,  # é—œé–‰æ¢¯åº¦æª¢æŸ¥é»é¿å…ç›¸å®¹æ€§å•é¡Œ
        dataloader_num_workers=0,  # é—œé–‰å¤šé€²ç¨‹é¿å… Windows å•é¡Œ
        dataloader_pin_memory=True,  # å•Ÿç”¨ pin memory
        # æ—¥èªŒå’Œç›£æ§
        logging_steps=25,  # è©³ç´°çš„æ—¥èªŒè¨˜éŒ„
        report_to=["tensorboard"],  # å•Ÿç”¨ TensorBoard
        # å…¶ä»–è¨­å®š
        remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰æ¬„ä½
        label_names=["labels"],  # æŒ‡å®šæ¨™ç±¤åç¨±
        push_to_hub=False,  # ä¸æ¨é€åˆ° Hub
        # å„ªåŒ–å™¨è¨­å®š
        optim="adamw_torch",  # ä½¿ç”¨ PyTorch AdamW
        weight_decay=0.01,  # é©åº¦çš„æ¬Šé‡è¡°æ¸›
        lr_scheduler_type="cosine",  # ä½¿ç”¨ cosine èª¿åº¦å™¨
        # ç§»é™¤æ—©åœè¨­å®š (Seq2SeqTrainingArguments ä¸æ”¯æ´)
    )

    # å‰µå»º Trainer
    print("\nğŸ‹ï¸  å‰µå»ºè¨“ç·´å™¨...")

    # å‰µå»ºå¸¶æœ‰ processor çš„ compute_metrics å‡½æ•¸
    def compute_metrics_with_processor(pred):
        return compute_metrics_optimal(pred, processor)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics_with_processor,
        tokenizer=processor.feature_extractor,
    )

    # è¨“ç·´å‰çš„è¨˜æ†¶é«”æª¢æŸ¥
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"   è¨“ç·´å‰ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")

    # é–‹å§‹è¨“ç·´
    print("\nğŸš€ é–‹å§‹è¨“ç·´...")
    print("=" * 60)

    try:
        # åŸ·è¡Œè¨“ç·´
        trainer.train()

        print("\nğŸ‰ è¨“ç·´å®Œæˆï¼")

        # ä¿å­˜æœ€çµ‚æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)

        # æœ€çµ‚è©•ä¼°
        print("\nğŸ“Š æœ€çµ‚è©•ä¼°...")
        final_metrics = trainer.evaluate()

        print(f"   æœ€çµ‚ WER: {final_metrics.get('eval_wer', 'N/A'):.2f}%")
        if "eval_cer" in final_metrics:
            print(f"   æœ€çµ‚ CER: {final_metrics['eval_cer']:.2f}%")

        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
        print("ğŸ¯ è¨“ç·´æˆåŠŸå®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()

        # å˜—è©¦ä¿å­˜ç•¶å‰ç‹€æ…‹
        try:
            print("\nğŸ”„ å˜—è©¦ä¿å­˜ç•¶å‰ç‹€æ…‹...")
            trainer.save_model(f"{OUTPUT_DIR}_interrupted")
            processor.save_pretrained(f"{OUTPUT_DIR}_interrupted")
            print("   âœ… ä¸­æ–·ç‹€æ…‹å·²ä¿å­˜")
        except:
            print("   âŒ ç„¡æ³•ä¿å­˜ä¸­æ–·ç‹€æ…‹")

        return

    # æ¸…ç†è¨˜æ†¶é«”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(
            f"\nğŸ§¹ æ¸…ç†å¾Œ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1024**3:.2f} GB"
        )


if __name__ == "__main__":
    main()
