#!/usr/bin/env python3
"""
Breeze-ASR-25 Google Colab æœ€ä½³åŒ–å¾®èª¿ç‰ˆæœ¬
é‡å° Colab T4/V100/L4/A100 GPU ç’°å¢ƒå„ªåŒ–
è‡ªå‹•åµæ¸¬ GPU é¡å‹ä¸¦èª¿æ•´æœ€ä½³åƒæ•¸
"""

import multiprocessing
import os
from dataclasses import dataclass
from functools import partial
from pathlib import Path
from typing import Dict, List, Optional, Union

import jiwer
import librosa
import numpy as np
import pandas as pd
import torch
from datasets import Audio, Dataset, DatasetDict
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperTokenizer,
)

# ==============================================================================
# Colab GPU ç’°å¢ƒé…ç½®
# ==============================================================================

MODEL_ID = "MediaTek-Research/Breeze-ASR-25"
DATASET_DIR = Path("./")
TRAIN_CSV = DATASET_DIR / "metadata_train.csv"
TEST_CSV = DATASET_DIR / "metadata_test.csv"
OUTPUT_DIR = "./breeze-asr-25-colab-optimized"

# å…¨åŸŸ processor è®Šæ•¸
_processor = None


def detect_gpu_type():
    """åµæ¸¬ Colab GPU é¡å‹ä¸¦è¿”å›æœ€ä½³é…ç½®"""
    if not torch.cuda.is_available():
        return "cpu", {}

    gpu_name = torch.cuda.get_device_name(0).lower()
    memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3

    print(f"ğŸ” åµæ¸¬åˆ° GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {memory_gb:.1f} GB")

    # æ ¹æ“š GPU é¡å‹é…ç½®æœ€ä½³åƒæ•¸
    if "k80" in gpu_name:
        return "k80", {
            "batch_size": 1,
            "gradient_accumulation_steps": 8,
            "fp16": False,  # K80 ä¸æ”¯æ´ FP16
            "max_steps": 2000,
            "learning_rate": 5e-6,
            "warmup_steps": 200,
            "save_steps": 200,
            "eval_steps": 200,
        }
    elif "t4" in gpu_name:
        return "t4", {
            "batch_size": 2,
            "gradient_accumulation_steps": 8,
            "fp16": True,
            "max_steps": 3000,
            "learning_rate": 1e-5,
            "warmup_steps": 300,
            "save_steps": 300,
            "eval_steps": 300,
        }
    elif "p100" in gpu_name:
        return "p100", {
            "batch_size": 3,
            "gradient_accumulation_steps": 6,
            "fp16": True,
            "max_steps": 4000,
            "learning_rate": 1e-5,
            "warmup_steps": 400,
            "save_steps": 400,
            "eval_steps": 400,
        }
    elif "v100" in gpu_name:
        return "v100", {
            "batch_size": 4,
            "gradient_accumulation_steps": 4,
            "fp16": True,
            "max_steps": 5000,
            "learning_rate": 2e-5,
            "warmup_steps": 500,
            "save_steps": 500,
            "eval_steps": 500,
        }
    elif "l4" in gpu_name:
        return "l4", {
            "batch_size": 6,
            "gradient_accumulation_steps": 3,
            "fp16": True,
            "max_steps": 6000,
            "learning_rate": 2e-5,
            "warmup_steps": 600,
            "save_steps": 600,
            "eval_steps": 600,
        }
    elif "a100" in gpu_name:
        return "a100", {
            "batch_size": 8,
            "gradient_accumulation_steps": 2,
            "fp16": True,
            "max_steps": 8000,
            "learning_rate": 3e-5,
            "warmup_steps": 800,
            "save_steps": 800,
            "eval_steps": 800,
        }
    else:
        # é è¨­é…ç½®
        return "unknown", {
            "batch_size": 2,
            "gradient_accumulation_steps": 8,
            "fp16": True,
            "max_steps": 3000,
            "learning_rate": 1e-5,
            "warmup_steps": 300,
            "save_steps": 300,
            "eval_steps": 300,
        }


def setup_colab_environment():
    """è¨­å®š Colab ç’°å¢ƒæœ€ä½³åŒ–"""
    # Colab è¨˜æ†¶é«”æœ€ä½³åŒ–
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = (
        "expandable_segments:True,max_split_size_mb:512"
    )
    os.environ["TOKENIZERS_PARALLELISM"] = "false"

    # æ¸…ç†è¨˜æ†¶é«”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    print("ğŸ”§ Colab ç’°å¢ƒè¨­å®šå®Œæˆ")


def init_processor(proc):
    """åˆå§‹åŒ–å…¨åŸŸ processor"""
    global _processor
    _processor = proc


# ==============================================================================
# è³‡æ–™è™•ç†å‡½æ•¸ (Colab æœ€ä½³åŒ–ç‰ˆæœ¬)
# ==============================================================================


def prepare_dataset_colab(batch, processor=None):
    """
    Colab æœ€ä½³åŒ–çš„å³æ™‚è³‡æ–™è½‰æ›å‡½æ•¸
    é‡å° Colab çš„è¨˜æ†¶é«”é™åˆ¶é€²è¡Œå„ªåŒ–
    """
    import gc

    import librosa
    import numpy as np

    # ä½¿ç”¨å‚³å…¥çš„ processor æˆ–å…¨åŸŸ processor
    current_processor = processor if processor is not None else _processor

    if current_processor is None:
        print("éŒ¯èª¤ï¼šç„¡æ³•å–å¾— processor")
        return batch

    # è™•ç†éŸ³è¨Šè³‡æ–™ - åˆ†æ‰¹è™•ç†æ¸›å°‘è¨˜æ†¶é«”å³°å€¼
    audio_list = []
    batch_files = batch["file"] if isinstance(batch["file"], list) else [batch["file"]]

    for audio_path in batch_files:
        try:
            # è¼‰å…¥éŸ³è¨Šä¸¦ç¢ºä¿ 16kHz æ¡æ¨£ç‡
            audio_array, sampling_rate = librosa.load(
                audio_path, sr=16000, duration=30
            )  # é™åˆ¶ 30 ç§’

            # æ­£è¦åŒ–éŸ³è¨Š
            if len(audio_array) > 0:
                audio_array = audio_array / (np.abs(audio_array).max() + 1e-8)

            audio_list.append({"array": audio_array, "sampling_rate": sampling_rate})
        except Exception as e:
            print(f"è­¦å‘Šï¼šè¼‰å…¥éŸ³è¨Šå¤±æ•— {audio_path}: {e}")
            # ä½¿ç”¨è¼ƒçŸ­çš„éœéŸ³æ›¿ä»£
            audio_list.append({"array": np.zeros(8000), "sampling_rate": 16000})

    # è™•ç†éŸ³è¨Šç‰¹å¾µ - ä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡
    try:
        # åˆ†æ‰¹è™•ç†éŸ³è¨Šç‰¹å¾µä»¥é¿å…è¨˜æ†¶é«”æº¢å‡º
        feature_list = []
        for audio in audio_list:
            features = current_processor.feature_extractor(
                audio["array"],
                sampling_rate=16000,
                return_tensors="np",
                max_length=3000,  # é™åˆ¶åºåˆ—é•·åº¦
                truncation=True,
            ).input_features[0]
            feature_list.append(features)

        batch["input_features"] = (
            np.stack(feature_list) if len(feature_list) > 1 else np.array(feature_list)
        )

    except Exception as e:
        print(f"éŸ³è¨Šç‰¹å¾µæå–å¤±æ•—: {e}")
        batch_size = len(audio_list)
        batch["input_features"] = np.zeros((batch_size, 80, 3000))

    # è™•ç†æ–‡å­—æ¨™ç±¤
    try:
        texts = (
            batch["ä¸­æ–‡æ„è­¯"]
            if isinstance(batch["ä¸­æ–‡æ„è­¯"], list)
            else [batch["ä¸­æ–‡æ„è­¯"]]
        )
        texts = [
            str(text).strip() if text and str(text).strip() else "ç„¡å…§å®¹"
            for text in texts
        ]

        labels = current_processor.tokenizer(
            texts,
            max_length=256,  # é™ä½æœ€å¤§é•·åº¦ä»¥ç¯€çœè¨˜æ†¶é«”
            truncation=True,
            padding=False,  # ä¸åœ¨é€™è£¡å¡«å……
            return_tensors="np",
        ).input_ids

        batch["labels"] = labels

    except Exception as e:
        print(f"æ–‡å­—æ¨™ç±¤è™•ç†å¤±æ•—: {e}")
        batch["labels"] = np.array([[1]] * len(audio_list))

    # æ‰‹å‹•åƒåœ¾å›æ”¶
    gc.collect()

    return batch


@dataclass
class DataCollatorSpeechSeq2SeqColab:
    """
    Colab æœ€ä½³åŒ–çš„ Data Collator
    é‡å°è¨˜æ†¶é«”æ•ˆç‡å„ªåŒ–
    """

    processor: WhisperProcessor
    decoder_start_token_id: Optional[int] = None

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:

        # è™•ç† input_features - ç¢ºä¿çµ±ä¸€æ ¼å¼
        input_features = []
        for feature in features:
            if isinstance(feature["input_features"], np.ndarray):
                if feature["input_features"].ndim == 2:
                    # å¦‚æœæ˜¯ 2Dï¼Œæ·»åŠ æ‰¹æ¬¡ç¶­åº¦
                    input_feat = torch.tensor(
                        feature["input_features"], dtype=torch.float32
                    ).unsqueeze(0)
                else:
                    input_feat = torch.tensor(
                        feature["input_features"], dtype=torch.float32
                    )
            else:
                input_feat = torch.tensor(
                    feature["input_features"], dtype=torch.float32
                )

            input_features.append({"input_features": input_feat.squeeze(0)})

        # ä½¿ç”¨ processor é€²è¡Œå¡«å……
        try:
            batch = self.processor.feature_extractor.pad(
                input_features, return_tensors="pt", max_length=3000, truncation=True
            )
        except Exception as e:
            print(f"ç‰¹å¾µå¡«å……å¤±æ•—: {e}")
            # æ‰‹å‹•å‰µå»ºæ‰¹æ¬¡
            max_len = 3000
            batch_input_features = torch.zeros((len(features), 80, max_len))
            for i, feature in enumerate(features):
                feat = torch.tensor(feature["input_features"], dtype=torch.float32)
                if feat.ndim == 3:
                    feat = feat.squeeze(0)
                seq_len = min(feat.shape[-1], max_len)
                batch_input_features[i, :, :seq_len] = feat[:, :seq_len]
            batch = {"input_features": batch_input_features}

        # è™•ç†æ¨™ç±¤
        label_features = []
        for feature in features:
            labels = feature["labels"]
            if isinstance(labels, np.ndarray):
                if labels.ndim == 2:
                    labels = labels[0]  # å–ç¬¬ä¸€å€‹åºåˆ—
                labels = torch.tensor(labels, dtype=torch.long)
            else:
                labels = torch.tensor(labels, dtype=torch.long)
            label_features.append({"input_ids": labels})

        try:
            labels_batch = self.processor.tokenizer.pad(
                label_features, return_tensors="pt", max_length=256, truncation=True
            )
        except Exception as e:
            print(f"æ¨™ç±¤å¡«å……å¤±æ•—: {e}")
            # æ‰‹å‹•å‰µå»ºæ¨™ç±¤æ‰¹æ¬¡
            max_label_len = 256
            batch_labels = torch.full(
                (len(features), max_label_len), self.processor.tokenizer.pad_token_id
            )
            attention_mask = torch.zeros((len(features), max_label_len))

            for i, feature in enumerate(features):
                labels = torch.tensor(feature["labels"], dtype=torch.long)
                if labels.ndim > 1:
                    labels = labels.flatten()
                label_len = min(len(labels), max_label_len)
                batch_labels[i, :label_len] = labels[:label_len]
                attention_mask[i, :label_len] = 1

            labels_batch = {"input_ids": batch_labels, "attention_mask": attention_mask}

        # è™•ç†æ¨™ç±¤é®ç½©
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"].ne(1), -100
        )

        # ç§»é™¤ BOS tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if self.decoder_start_token_id is not None:
            if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():
                labels = labels[:, 1:]

        batch["labels"] = labels
        return batch


def compute_metrics_colab(pred, processor=None):
    """
    Colab æœ€ä½³åŒ–çš„è©•ä¼°æŒ‡æ¨™è¨ˆç®—
    """
    current_processor = processor if processor is not None else _processor

    if current_processor is None:
        print("éŒ¯èª¤ï¼šç„¡æ³•å–å¾— processor é€²è¡Œè©•ä¼°")
        return {"wer": 100.0}

    pred_ids = pred.predictions
    label_ids = pred.label_ids

    # å°‡ -100 æ›¿æ›ç‚º pad token
    label_ids[label_ids == -100] = current_processor.tokenizer.pad_token_id

    # è§£ç¢¼é æ¸¬å’Œæ¨™ç±¤
    try:
        pred_str = current_processor.tokenizer.batch_decode(
            pred_ids, skip_special_tokens=True
        )
        label_str = current_processor.tokenizer.batch_decode(
            label_ids, skip_special_tokens=True
        )

        # è¨ˆç®— WERï¼ˆç™¾åˆ†æ¯”ï¼‰
        wer = 100 * jiwer.wer(label_str, pred_str)

        # è¨ˆç®—é¡å¤–æŒ‡æ¨™
        try:
            cer = 100 * jiwer.cer(label_str, pred_str)
            return {"wer": wer, "cer": cer}
        except:
            return {"wer": wer}

    except Exception as e:
        print(f"è©•ä¼°æŒ‡æ¨™è¨ˆç®—å¤±æ•—: {e}")
        return {"wer": 100.0}


# ==============================================================================
# ä¸»è¦è¨“ç·´å‡½æ•¸
# ==============================================================================


def main():
    """
    Colab æœ€ä½³åŒ–çš„ä¸»è¦è¨“ç·´æµç¨‹
    """
    global _processor

    print("ğŸš€ Breeze-ASR-25 Google Colab æœ€ä½³åŒ–å¾®èª¿é–‹å§‹")
    print("=" * 60)

    # è¨­å®š Colab ç’°å¢ƒ
    setup_colab_environment()

    # åµæ¸¬ GPU ä¸¦å–å¾—æœ€ä½³é…ç½®
    gpu_type, gpu_config = detect_gpu_type()
    print(f"ğŸ¯ ä½¿ç”¨ {gpu_type.upper()} æœ€ä½³åŒ–é…ç½®")

    # æª¢æŸ¥è³‡æ–™æª”æ¡ˆ
    if not TRAIN_CSV.exists() or not TEST_CSV.exists():
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°è³‡æ–™æª”æ¡ˆ")
        print(f"   è¨“ç·´æª”æ¡ˆ: {TRAIN_CSV}")
        print(f"   æ¸¬è©¦æª”æ¡ˆ: {TEST_CSV}")
        return

    # è¼‰å…¥è³‡æ–™
    print("\nğŸ“Š è¼‰å…¥è³‡æ–™é›†...")
    train_df = pd.read_csv(TRAIN_CSV)
    test_df = pd.read_csv(TEST_CSV)

    print(f"   åŸå§‹è¨“ç·´è³‡æ–™: {len(train_df)} ç­†")
    print(f"   åŸå§‹æ¸¬è©¦è³‡æ–™: {len(test_df)} ç­†")

    # è‡ªå‹•æª¢æŸ¥å’Œä¿®æ­£è·¯å¾‘æ ¼å¼
    print("\nğŸ”§ æª¢æŸ¥è·¯å¾‘æ ¼å¼...")
    path_column = "file"  # å‡è¨­è·¯å¾‘åœ¨ 'file' æ¬„ä½

    if path_column in train_df.columns:
        sample_path = str(train_df[path_column].iloc[0]).strip()

        # æª¢æŸ¥æ˜¯å¦ç‚º Windows çµ•å°è·¯å¾‘
        if sample_path.startswith(("C:", "D:", "E:", "F:")):
            print(f"   âš ï¸  åµæ¸¬åˆ° Windows çµ•å°è·¯å¾‘: {sample_path}")
            print("   ğŸ”„ è‡ªå‹•ä¿®æ­£ç‚º Colab è·¯å¾‘...")

            # ä¿®æ­£è¨“ç·´è³‡æ–™è·¯å¾‘
            train_df[path_column] = train_df[path_column].apply(
                lambda x: (
                    f"/content/drive/MyDrive/audio_model/audio_files/{Path(x).name}"
                    if isinstance(x, str) and x.startswith(("C:", "D:", "E:", "F:"))
                    else x
                )
            )

            # ä¿®æ­£æ¸¬è©¦è³‡æ–™è·¯å¾‘
            test_df[path_column] = test_df[path_column].apply(
                lambda x: (
                    f"/content/drive/MyDrive/audio_model/audio_files/{Path(x).name}"
                    if isinstance(x, str) and x.startswith(("C:", "D:", "E:", "F:"))
                    else x
                )
            )

            print(f"   âœ… è·¯å¾‘å·²ä¿®æ­£ï¼Œç¯„ä¾‹: {train_df[path_column].iloc[0]}")

        elif sample_path.startswith("/content/drive/"):
            print(f"   âœ… å·²æ˜¯æ­£ç¢ºçš„ Colab è·¯å¾‘: {sample_path}")

        else:
            print(f"   âš ï¸  è·¯å¾‘æ ¼å¼å¯èƒ½éœ€è¦èª¿æ•´: {sample_path}")
            print("   ğŸ’¡ å»ºè­°ä½¿ç”¨ fix_csv_paths_for_colab.py é å…ˆè™•ç†")

    # è³‡æ–™å“è³ªæª¢æŸ¥å’Œæ¸…ç†
    print("\nğŸ” è³‡æ–™å“è³ªæª¢æŸ¥...")
    required_columns = ["file", "ä¸­æ–‡æ„è­¯"]
    for col in required_columns:
        if col not in train_df.columns:
            print(f"âŒ éŒ¯èª¤ï¼šè¨“ç·´è³‡æ–™ç¼ºå°‘æ¬„ä½ '{col}'")
            return

    # ç§»é™¤ç©ºå€¼å’Œç„¡æ•ˆè³‡æ–™
    train_df = train_df.dropna(subset=required_columns)
    test_df = test_df.dropna(subset=required_columns)

    # éæ¿¾éé•·çš„æ–‡å­—ï¼ˆé¿å…è¨˜æ†¶é«”å•é¡Œï¼‰
    train_df = train_df[train_df["ä¸­æ–‡æ„è­¯"].str.len() < 200]
    test_df = test_df[test_df["ä¸­æ–‡æ„è­¯"].str.len() < 200]

    print(f"   æ¸…ç†å¾Œè¨“ç·´è³‡æ–™: {len(train_df)} ç­†")
    print(f"   æ¸…ç†å¾Œæ¸¬è©¦è³‡æ–™: {len(test_df)} ç­†")

    # é‡å°è¼ƒæ…¢çš„ GPU æ¸›å°‘è³‡æ–™é‡
    if gpu_type in ["k80", "t4"]:
        train_sample_size = min(len(train_df), 20000)  # é™åˆ¶è¨“ç·´è³‡æ–™é‡
        test_sample_size = min(len(test_df), 2000)

        train_df = train_df.sample(n=train_sample_size, random_state=42)
        test_df = test_df.sample(n=test_sample_size, random_state=42)

        print(
            f"   GPU è¨˜æ†¶é«”é™åˆ¶ï¼Œä½¿ç”¨å­é›†ï¼šè¨“ç·´ {len(train_df)} ç­†ï¼Œæ¸¬è©¦ {len(test_df)} ç­†"
        )

    # è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨
    print("\nğŸ¤– è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨...")
    try:
        processor = WhisperProcessor.from_pretrained(MODEL_ID)

        # æ ¹æ“š GPU é…ç½®é¸æ“‡ç²¾åº¦
        dtype = torch.float32 if not gpu_config.get("fp16", True) else torch.float16

        model = WhisperForConditionalGeneration.from_pretrained(
            MODEL_ID,
            torch_dtype=dtype,
            low_cpu_mem_usage=True,
            use_cache=False,  # ç¯€çœè¨˜æ†¶é«”
        )

        # æ¨¡å‹é…ç½®å„ªåŒ–
        model.config.forced_decoder_ids = None
        model.config.suppress_tokens = []

        # å¦‚æœä¸æ”¯æ´ FP16ï¼Œç¢ºä¿ä½¿ç”¨ FP32
        if not gpu_config.get("fp16", True):
            model = model.float()

        print("   âœ… æ¨¡å‹å’Œè™•ç†å™¨è¼‰å…¥æˆåŠŸ")
        init_processor(processor)

    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        return

    # å‰µå»ºè³‡æ–™é›†
    print("\nğŸ“‹ è¨­å®šè³‡æ–™é›†å’Œå³æ™‚è½‰æ›...")
    try:
        # åˆä½µä¸¦é‡æ–°åˆ†å‰²è³‡æ–™
        full_df = pd.concat([train_df, test_df], ignore_index=True)
        full_dataset = Dataset.from_pandas(full_df)

        dataset_split = full_dataset.train_test_split(test_size=0.2, seed=42)

        def prepare_dataset_with_processor(batch):
            return prepare_dataset_colab(batch, processor)

        vectorized_datasets = dataset_split.with_transform(
            prepare_dataset_with_processor
        )

        train_dataset = vectorized_datasets["train"]
        test_dataset = vectorized_datasets["test"]

        print(f"   âœ… å³æ™‚è½‰æ›è¨­å®šå®Œæˆ")
        print(f"   æœ€çµ‚è¨“ç·´è³‡æ–™: {len(train_dataset)} ç­†")
        print(f"   æœ€çµ‚æ¸¬è©¦è³‡æ–™: {len(test_dataset)} ç­†")

    except Exception as e:
        print(f"âŒ è³‡æ–™é›†è¨­å®šå¤±æ•—: {e}")
        return

    # å‰µå»º Data Collator
    data_collator = DataCollatorSpeechSeq2SeqColab(
        processor=processor,
        decoder_start_token_id=model.config.decoder_start_token_id,
    )

    # è¨­å®šè¨“ç·´åƒæ•¸ï¼ˆä½¿ç”¨ GPU ç‰¹å®šé…ç½®ï¼‰
    print(f"\nâš™ï¸  é…ç½® {gpu_type.upper()} æœ€ä½³åŒ–è¨“ç·´åƒæ•¸...")
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        # GPU ç‰¹å®šçš„æ‰¹æ¬¡è¨­å®š
        per_device_train_batch_size=gpu_config["batch_size"],
        per_device_eval_batch_size=max(1, gpu_config["batch_size"] // 2),
        gradient_accumulation_steps=gpu_config["gradient_accumulation_steps"],
        # å­¸ç¿’ç‡å’Œèª¿åº¦
        learning_rate=gpu_config["learning_rate"],
        warmup_steps=gpu_config["warmup_steps"],
        max_steps=gpu_config["max_steps"],
        # è©•ä¼°å’Œç”Ÿæˆè¨­å®š
        eval_strategy="steps",
        eval_steps=gpu_config["eval_steps"],
        predict_with_generate=True,
        generation_max_length=256,  # é™ä½ç”Ÿæˆé•·åº¦
        # ä¿å­˜ç­–ç•¥
        save_steps=gpu_config["save_steps"],
        save_total_limit=2,  # åªä¿ç•™ 2 å€‹æª¢æŸ¥é»ç¯€çœç©ºé–“
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        # è¨˜æ†¶é«”å’Œæ•ˆèƒ½å„ªåŒ–
        fp16=gpu_config["fp16"],
        gradient_checkpointing=False,  # é—œé–‰æ¢¯åº¦æª¢æŸ¥é»
        dataloader_num_workers=0,  # Colab å»ºè­°è¨­ç‚º 0
        dataloader_pin_memory=False,  # Colab ç’°å¢ƒå»ºè­°é—œé–‰
        # æ—¥èªŒå’Œç›£æ§
        logging_steps=50,
        report_to=[],  # é—œé–‰å¤–éƒ¨å ±å‘Šä»¥ç¯€çœè¨˜æ†¶é«”
        # å…¶ä»–è¨­å®š
        remove_unused_columns=False,
        label_names=["labels"],
        push_to_hub=False,
        # å„ªåŒ–å™¨è¨­å®š
        optim="adamw_torch",
        weight_decay=0.01,
        lr_scheduler_type="cosine",
        # Colab ç‰¹æ®Šè¨­å®š
        ignore_data_skip=True,  # å¿½ç•¥è³‡æ–™è·³é
        dataloader_drop_last=True,  # ä¸Ÿæ£„æœ€å¾Œä¸å®Œæ•´æ‰¹æ¬¡
    )

    # è¨˜æ†¶é«”æ¸…ç†
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # å‰µå»º Trainer
    print("\nğŸ‹ï¸  å‰µå»ºè¨“ç·´å™¨...")

    def compute_metrics_with_processor(pred):
        return compute_metrics_colab(pred, processor)

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics_with_processor,
        tokenizer=processor.feature_extractor,
    )

    # é¡¯ç¤ºè¨“ç·´è³‡è¨Š
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    effective_batch_size = (
        gpu_config["batch_size"] * gpu_config["gradient_accumulation_steps"]
    )

    print(f"\nğŸ“ˆ è¨“ç·´è³‡è¨Š:")
    print(f"   æ¨¡å‹åƒæ•¸ç¸½æ•¸: {total_params:,}")
    print(f"   å¯è¨“ç·´åƒæ•¸: {trainable_params:,}")
    print(f"   æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
    print(f"   é ä¼°è¨“ç·´æ™‚é–“: {gpu_type.upper()} ç´„ 1-3 å°æ™‚")

    # é–‹å§‹è¨“ç·´
    print("\nğŸš€ é–‹å§‹è¨“ç·´...")
    print("=" * 60)

    try:
        # è¨“ç·´å‰è¨˜æ†¶é«”æª¢æŸ¥
        if torch.cuda.is_available():
            print(
                f"   è¨“ç·´å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**3:.2f} GB"
            )

        # åŸ·è¡Œè¨“ç·´
        trainer.train()

        print("\nğŸ‰ è¨“ç·´å®Œæˆï¼")

        # ä¿å­˜æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
        trainer.save_model(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)

        # æœ€çµ‚è©•ä¼°
        print("\nğŸ“Š æœ€çµ‚è©•ä¼°...")
        final_metrics = trainer.evaluate()

        print(f"   æœ€çµ‚ WER: {final_metrics.get('eval_wer', 'N/A'):.2f}%")
        if "eval_cer" in final_metrics:
            print(f"   æœ€çµ‚ CER: {final_metrics['eval_cer']:.2f}%")

        print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")
        print("ğŸ¯ Colab è¨“ç·´æˆåŠŸå®Œæˆï¼")

        # æä¾›ä¸‹è¼‰æŒ‡ä»¤
        print(f"\nğŸ“¦ æ¨¡å‹ä¸‹è¼‰æŒ‡ä»¤:")
        print(f"!zip -r {OUTPUT_DIR}.zip {OUTPUT_DIR}")
        print(f"from google.colab import files")
        print(f"files.download('{OUTPUT_DIR}.zip')")

    except Exception as e:
        print(f"\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback

        traceback.print_exc()

        # ä¿å­˜ä¸­æ–·ç‹€æ…‹
        try:
            print("\nğŸ”„ å˜—è©¦ä¿å­˜ç•¶å‰ç‹€æ…‹...")
            trainer.save_model(f"{OUTPUT_DIR}_interrupted")
            processor.save_pretrained(f"{OUTPUT_DIR}_interrupted")
            print("   âœ… ä¸­æ–·ç‹€æ…‹å·²ä¿å­˜")
        except:
            print("   âŒ ç„¡æ³•ä¿å­˜ä¸­æ–·ç‹€æ…‹")

    finally:
        # æ¸…ç†è¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(
                f"\nğŸ§¹ æ¸…ç†å¾Œ GPU è¨˜æ†¶é«”: {torch.cuda.memory_allocated() / 1024**3:.2f} GB"
            )


# ==============================================================================
# Colab è¼”åŠ©å‡½æ•¸
# ==============================================================================


def check_colab_environment():
    """æª¢æŸ¥ Colab ç’°å¢ƒä¸¦æä¾›å»ºè­°"""
    print("ğŸ” Colab ç’°å¢ƒæª¢æŸ¥:")

    # GPU æª¢æŸ¥
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   âœ… GPU: {gpu_name} ({memory_gb:.1f} GB)")
    else:
        print("   âŒ æœªæª¢æ¸¬åˆ° GPUï¼Œè«‹ç¢ºèªå·²å•Ÿç”¨ GPU é‹è¡Œæ™‚")
        return False

    # RAM æª¢æŸ¥
    import psutil

    ram_gb = psutil.virtual_memory().total / 1024**3
    print(f"   âœ… RAM: {ram_gb:.1f} GB")

    # ç£ç¢Ÿç©ºé–“æª¢æŸ¥
    disk_usage = psutil.disk_usage("/")
    free_gb = disk_usage.free / 1024**3
    print(f"   âœ… å¯ç”¨ç£ç¢Ÿç©ºé–“: {free_gb:.1f} GB")

    if free_gb < 5:
        print("   âš ï¸  è­¦å‘Šï¼šç£ç¢Ÿç©ºé–“ä¸è¶³ï¼Œå»ºè­°æ¸…ç†å¾Œå†è¨“ç·´")

    return True


def setup_colab_training():
    """è¨­å®š Colab è¨“ç·´ç’°å¢ƒçš„å®Œæ•´æµç¨‹"""
    print("ğŸ”§ è¨­å®š Colab è¨“ç·´ç’°å¢ƒ...")

    # æ›è¼‰ Google Driveï¼ˆå¦‚æœéœ€è¦ï¼‰
    try:
        from google.colab import drive

        drive.mount("/content/drive")
        print("   âœ… Google Drive å·²æ›è¼‰")
    except:
        print("   â„¹ï¸  æœªæ›è¼‰ Google Driveï¼ˆé Colab ç’°å¢ƒæˆ–å·²æ›è¼‰ï¼‰")

    # å®‰è£å¿…è¦å¥—ä»¶
    required_packages = [
        "datasets",
        "transformers",
        "accelerate",
        "jiwer",
        "librosa",
        "soundfile",
    ]

    print("   ğŸ“¦ æª¢æŸ¥å¿…è¦å¥—ä»¶...")
    for package in required_packages:
        try:
            __import__(package)
            print(f"      âœ… {package}")
        except ImportError:
            print(f"      âš ï¸  {package} éœ€è¦å®‰è£")
            print(f"         è«‹åŸ·è¡Œ: !pip install {package}")


if __name__ == "__main__":
    # Colab ç’°å¢ƒæª¢æŸ¥
    if not check_colab_environment():
        print("âŒ ç’°å¢ƒæª¢æŸ¥å¤±æ•—ï¼Œè«‹ä¿®æ­£å¾Œé‡è©¦")
        exit(1)

    # è¨­å®šè¨“ç·´ç’°å¢ƒ
    setup_colab_training()

    # åŸ·è¡Œä¸»è¨“ç·´
    main()
