{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb24ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset,DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# å‡è¨­ä½ çš„è³‡æ–™é›†æ˜¯ CSV æ ¼å¼\n",
    "data = pd.read_csv('output/final_audio_paths.csv')\n",
    "\n",
    "# å°‡è³‡æ–™è½‰æ›ç‚º Hugging Face Dataset æ ¼å¼\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"test\": Dataset.from_pandas(data.sample(frac=0.2))  # éš¨æ©Ÿå– 20% ä½œç‚ºæ¸¬è©¦é›†\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f291f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any\n",
    "\n",
    "class AudioDatasetProcessor_V3:\n",
    "    \"\"\"\n",
    "    è™•ç†éŸ³è¨Š CSV çš„æœ€çµ‚ç‰ˆæœ¬é¡åˆ¥ã€‚\n",
    "    - è®€å– 'path' èˆ‡ 'transcription' æ¬„ä½ã€‚\n",
    "    - å°‡ 'transcription' é‡æ–°å‘½åç‚º 'sentence'ã€‚\n",
    "    - ç”¢ç”Ÿèˆ‡ç›®æ¨™æ ¼å¼å®Œå…¨åŒ¹é…çš„ DatasetDictã€‚\n",
    "\n",
    "    å±¬æ€§:\n",
    "        file_path (str): è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "        target_sampling_rate (int): ç›®æ¨™éŸ³è¨Šå–æ¨£ç‡ã€‚\n",
    "        test_size (float): ç”¨æ–¼åŠƒåˆ†æ¸¬è©¦é›†çš„è³‡æ–™æ¯”ä¾‹ã€‚\n",
    "        random_state (int): éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿å¯é‡è¤‡çš„è³‡æ–™åˆ‡åˆ†ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, target_sampling_rate: int = 16000, test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"åˆå§‹åŒ– AudioDatasetProcessor_V3\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_single_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        \"\"\"\n",
    "        å°‡å–®ä¸€ DataFrame è½‰æ›ç‚ºå…·æœ‰æŒ‡å®šå–æ¨£ç‡ Audio ç‰¹å¾µçš„ Datasetã€‚\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): åŒ…å« 'audio' å’Œ 'sentence' æ¬„ä½çš„ DataFrameã€‚\n",
    "\n",
    "        Returns:\n",
    "            Dataset: å·²è™•ç†çš„ Dataset ç‰©ä»¶ã€‚\n",
    "        \"\"\"\n",
    "        # è¤‡è£½ 'path' æ¬„ä½åˆ° 'audio'ï¼Œæ­¤æ™‚ df æ‡‰åŒ…å« 'path' å’Œ 'sentence'\n",
    "        temp_df = df.copy()\n",
    "        temp_df['audio'] = temp_df['file']\n",
    "\n",
    "        dataset = Dataset.from_pandas(temp_df)\n",
    "        \n",
    "        # è½‰æ› audio æ¬„ä½å‹åˆ¥ä¸¦ç§»é™¤å¤šé¤˜çš„ path æ¬„ä½\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå®Œæ•´çš„è³‡æ–™è™•ç†æµç¨‹ï¼ŒåŒ…å«æ¬„ä½é‡æ–°å‘½åã€‚\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: åŒ…å« 'train' å’Œ 'test' splits çš„æœ€çµ‚ç‰©ä»¶ã€‚\n",
    "        \"\"\"\n",
    "        # è®€å– CSV\n",
    "        try:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {self.file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # é—œéµæ­¥é©Ÿ 1: æŒ‡å®šä¾†æºéœ€è¦çš„æ¬„ä½æ˜¯ 'path' å’Œ 'transcription'\n",
    "        required_columns = ['transcription', 'file']\n",
    "        if not all(col in full_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ã€‚éœ€è¦: {required_columns}\")\n",
    "        \n",
    "        filtered_data = full_data[required_columns]\n",
    "\n",
    "        # ä½¿ç”¨è™•ç†å¾Œçš„ DataFrame é€²è¡Œè³‡æ–™åˆ‡åˆ†\n",
    "        train_df, test_df = train_test_split(\n",
    "             filtered_data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # åˆ†åˆ¥ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†æº–å‚™ Dataset\n",
    "        train_dataset = self._prepare_single_dataset(train_df)\n",
    "        test_dataset = self._prepare_single_dataset(test_df)\n",
    "\n",
    "        final_dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        })\n",
    "\n",
    "        return final_dataset_dict\n",
    "\n",
    "# --- ä½¿ç”¨ç¯„ä¾‹ ---\n",
    "if __name__ == '__main__':\n",
    "    # å‡è¨­æ‚¨çš„ CSV æª”æ¡ˆåç‚º 'final_audio_paths.csv'\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    \n",
    "    # å¯¦ä¾‹åŒ–æœ€çµ‚ç‰ˆæœ¬çš„è™•ç†å™¨\n",
    "    # æ˜ç¢ºæŒ‡å®šå–æ¨£ç‡ç‚º 16000 Hz\n",
    "    processor_v3 = AudioDatasetProcessor_V3(file_path=CSV_PATH, target_sampling_rate=16000)\n",
    "    final_dataset = processor_v3.create_dataset_dict()\n",
    "\n",
    "    if final_dataset:\n",
    "        print(\"--- æœ€çµ‚ç‰ˆè³‡æ–™é›†çµæ§‹ ---\")\n",
    "        print(final_dataset)\n",
    "        \n",
    "        print(\"\\n--- Train Split Features ---\")\n",
    "        print(final_dataset['train'].features)\n",
    "\n",
    "        print(\"\\n--- æŠ½æŸ¥ç¬¬ä¸€ç­†è¨“ç·´è³‡æ–™ (é©—è­‰æœ€çµ‚æ ¼å¼) ---\")\n",
    "        print(final_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66819195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "class AudioDatasetProcessor_V3:\n",
    "    \"\"\"\n",
    "    è™•ç†éŸ³è¨Š CSV çš„æœ€çµ‚ç‰ˆæœ¬é¡åˆ¥ã€‚\n",
    "    - è®€å– 'path' èˆ‡ 'transcription' æ¬„ä½ã€‚\n",
    "    - å°‡ 'transcription' é‡æ–°å‘½åç‚º 'sentence'ã€‚\n",
    "    - ç”¢ç”Ÿèˆ‡ç›®æ¨™æ ¼å¼å®Œå…¨åŒ¹é…çš„ DatasetDictã€‚\n",
    "\n",
    "    å±¬æ€§:\n",
    "        file_path (str): è¼¸å…¥çš„ CSV æª”æ¡ˆè·¯å¾‘ã€‚\n",
    "        target_sampling_rate (int): ç›®æ¨™éŸ³è¨Šå–æ¨£ç‡ã€‚\n",
    "        test_size (float): ç”¨æ–¼åŠƒåˆ†æ¸¬è©¦é›†çš„è³‡æ–™æ¯”ä¾‹ã€‚\n",
    "        random_state (int): éš¨æ©Ÿç¨®å­ï¼Œç¢ºä¿å¯é‡è¤‡çš„è³‡æ–™åˆ‡åˆ†ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, target_sampling_rate: int = 16000, test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"åˆå§‹åŒ– AudioDatasetProcessor_V3\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_single_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        \"\"\"\n",
    "        å°‡å–®ä¸€ DataFrame è½‰æ›ç‚ºå…·æœ‰æŒ‡å®šå–æ¨£ç‡ Audio ç‰¹å¾µçš„ Datasetã€‚\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): åŒ…å« 'audio' å’Œ 'sentence' æ¬„ä½çš„ DataFrameã€‚\n",
    "\n",
    "        Returns:\n",
    "            Dataset: å·²è™•ç†çš„ Dataset ç‰©ä»¶ã€‚\n",
    "        \"\"\"\n",
    "        # è¤‡è£½ 'path' æ¬„ä½åˆ° 'audio'ï¼Œæ­¤æ™‚ df æ‡‰åŒ…å« 'path' å’Œ 'sentence'\n",
    "        temp_df = df.copy()\n",
    "        temp_df['audio'] = temp_df['file']\n",
    "\n",
    "        dataset = Dataset.from_pandas(temp_df)\n",
    "        \n",
    "        # è½‰æ› audio æ¬„ä½å‹åˆ¥ä¸¦ç§»é™¤å¤šé¤˜çš„ path æ¬„ä½\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå®Œæ•´çš„è³‡æ–™è™•ç†æµç¨‹ï¼ŒåŒ…å«æ¬„ä½é‡æ–°å‘½åã€‚\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: åŒ…å« 'train' å’Œ 'test' splits çš„æœ€çµ‚ç‰©ä»¶ã€‚\n",
    "        \"\"\"\n",
    "        # è®€å– CSV\n",
    "        try:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æª”æ¡ˆ {self.file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # é—œéµæ­¥é©Ÿ 1: æŒ‡å®šä¾†æºéœ€è¦çš„æ¬„ä½æ˜¯ 'path' å’Œ 'transcription'\n",
    "        required_columns = ['transcription', 'file']\n",
    "        if not all(col in full_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV æª”æ¡ˆç¼ºå°‘å¿…è¦æ¬„ä½ã€‚éœ€è¦: {required_columns}\")\n",
    "        \n",
    "        filtered_data = full_data[required_columns]\n",
    "\n",
    "        # ä½¿ç”¨è™•ç†å¾Œçš„ DataFrame é€²è¡Œè³‡æ–™åˆ‡åˆ†\n",
    "        train_df, test_df = train_test_split(\n",
    "             filtered_data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # åˆ†åˆ¥ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†æº–å‚™ Dataset\n",
    "        train_dataset = self._prepare_single_dataset(train_df)\n",
    "        test_dataset = self._prepare_single_dataset(test_df)\n",
    "\n",
    "        final_dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        })\n",
    "\n",
    "        return final_dataset_dict\n",
    "\n",
    "# --- ä½¿ç”¨ç¯„ä¾‹ ---\n",
    "if __name__ == '__main__':\n",
    "    # å‡è¨­æ‚¨çš„ CSV æª”æ¡ˆåç‚º 'final_audio_paths.csv'\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    \n",
    "    # å¯¦ä¾‹åŒ–æœ€çµ‚ç‰ˆæœ¬çš„è™•ç†å™¨\n",
    "    # æ˜ç¢ºæŒ‡å®šå–æ¨£ç‡ç‚º 16000 Hz\n",
    "    processor_v3 = AudioDatasetProcessor_V3(file_path=CSV_PATH, target_sampling_rate=16000)\n",
    "    final_dataset = processor_v3.create_dataset_dict()\n",
    "\n",
    "    if final_dataset:\n",
    "        print(\"--- æœ€çµ‚ç‰ˆè³‡æ–™é›†çµæ§‹ ---\")\n",
    "        print(final_dataset)\n",
    "        \n",
    "        print(\"\\n--- Train Split Features ---\")\n",
    "        print(final_dataset['train'].features)\n",
    "\n",
    "        print(\"\\n--- æŠ½æŸ¥ç¬¬ä¸€ç­†è¨“ç·´è³‡æ–™ (é©—è­‰æœ€çµ‚æ ¼å¼) ---\")\n",
    "        print(final_dataset['train'][0])\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"zh\", task=\"transcribe\")\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = final_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "input_str = final_dataset[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "final_dataset = final_dataset.map(prepare_dataset, remove_columns=final_dataset.column_names[\"train\"], num_proc=4)\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# print(f\"Input:                 {input_str}\")\n",
    "# print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "# print(f\"Decoded w/out special: {decoded_str}\")\n",
    "# print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b48cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafa7399599449fd96fa757c2289d767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6509c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­¥é©Ÿ 1/4: è¼‰å…¥ Processor å’Œæ¨¡å‹ ---\n",
      "\n",
      "--- æ­¥é©Ÿ 2/4: å»ºç«‹åŸå§‹è³‡æ–™é›†ä¸¦è¨­å®šã€å³æ™‚è½‰æ›ã€---\n",
      "å³æ™‚è½‰æ›å·²è¨­å®šã€‚\n",
      "\n",
      "--- æ­¥é©Ÿ 3/4: å»ºç«‹è¨“ç·´å…ƒä»¶ (æœ€çµ‚ç©©å®šé‹è¡Œç‰ˆ) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24512\\4225368149.py:153: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ­¥é©Ÿ 4/4: é–‹å§‹æ¨¡å‹å¾®èª¿è¨“ç·´ (ä½¿ç”¨æœ€çµ‚ç©©å®šé«˜æ•ˆæ¨¡å¼) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mæœ€çµ‚æ¨¡å‹å·²å„²å­˜è‡³ï¼š\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# ç¢ºä¿æ‚¨å·²åœ¨çµ‚ç«¯æ©Ÿä½¿ç”¨ `huggingface-cli login` ç™»å…¥\u001b[39;00m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# åŸ·è¡Œå‰å»ºè­°é‡æ–°å•Ÿå‹•æ‚¨çš„é›»è…¦ï¼Œç¢ºä¿ç³»çµ±è™•æ–¼ä¹¾æ·¨ç‹€æ…‹\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- æ­¥é©Ÿ 4/4: é–‹å§‹æ¨¡å‹å¾®èª¿è¨“ç·´ (ä½¿ç”¨æœ€çµ‚ç©©å®šé«˜æ•ˆæ¨¡å¼) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# ä¸å¸¶åƒæ•¸çš„ .train() æœƒè‡ªå‹•è™•ç†æ–·é»çºŒç·´ï¼Œæ˜¯æœ€ç©©å¥çš„åšæ³•ã€‚\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m*** è¨“ç·´å®Œæˆ ***\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# --- å„²å­˜æœ€çµ‚æ¨¡å‹ ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2197\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2195\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2196\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2197\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2204\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3797\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3795\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2549\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# æª”æ¡ˆï¼štrain_final.py\n",
    "# æè¿°ï¼šä¸€å€‹å®Œæ•´ã€é«˜æ•ˆã€ç©©å¥çš„ Whisper æ¨¡å‹å¾®èª¿æµç¨‹çš„æœ€çµ‚ç‰ˆæœ¬ã€‚\n",
    "# æ ¸å¿ƒç­–ç•¥ï¼š\n",
    "# 1. å³æ™‚è½‰æ› (.with_transform)ï¼šå¾¹åº•è§£æ±ºè¨˜æ†¶é«”ä¸è¶³èˆ‡é è™•ç†éä¹…çš„å•é¡Œã€‚\n",
    "# 2. èƒŒæ™¯é å– (dataloader_num_workers)ï¼šè§£æ±º CPU èˆ‡ I/O ç“¶é ¸ï¼Œæœ€å¤§åŒ– GPU ä½¿ç”¨ç‡ã€‚\n",
    "# 3. å…¨åŸŸå®šç¾© (Global Scope)ï¼šè§£æ±ºå¤šæ ¸å¿ƒè™•ç†æ™‚çš„ pickling éŒ¯èª¤ã€‚\n",
    "# 4. æ™ºæ…§çºŒç·´ (Smart Resuming)ï¼šè‡ªå‹•å¾ä¸Šæ¬¡çš„æª¢æŸ¥é»æ¢å¾©è¨“ç·´ã€‚\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Union\n",
    "from dataclasses import dataclass\n",
    "import evaluate\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "# --- Hugging Face ç›¸é—œå°å…¥ ---\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "\n",
    "# ==============================================================================\n",
    "# æ­¥é©Ÿ 1: å°‡æ‰€æœ‰è¼”åŠ©é¡åˆ¥èˆ‡å‡½å¼å®šç¾©åœ¨ã€Œå…¨åŸŸç¯„åœã€\n",
    "# é€™æ˜¯ç‚ºäº†ç¢ºä¿åœ¨ä½¿ç”¨ dataloader_num_workers > 0 æ™‚ï¼ŒèƒŒæ™¯ç¨‹åºå¯ä»¥æˆåŠŸåºåˆ—åŒ– (pickle) å®ƒå€‘ã€‚\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"è™•ç†èªéŸ³åˆ°åºåˆ—è³‡æ–™çš„ Data Collatorï¼Œè² è²¬å°‡æ¨£æœ¬æ•´ç†æˆæ‰¹æ¬¡ä¸¦é€²è¡Œå¡«å……ã€‚\"\"\"\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset_batched(batch, feature_extractor, tokenizer):\n",
    "    \"\"\"å°‡ä¸€æ‰¹éŸ³è¨Šå’Œæ–‡æœ¬è³‡æ–™ã€å³æ™‚ã€è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼ã€‚\"\"\"\n",
    "    audio_list = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        [x[\"array\"] for x in audio_list], \n",
    "        sampling_rate=audio_list[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"], max_length=448, truncation=True).input_ids\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(pred, tokenizer):\n",
    "    \"\"\"åœ¨è©•ä¼°éšæ®µï¼Œè¨ˆç®—ä¸¦å›å‚³ WER æŒ‡æ¨™ã€‚\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# ==============================================================================\n",
    "# æ­¥é©Ÿ 2: ä¸»åŸ·è¡Œæµç¨‹\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    # --- åƒæ•¸è¨­å®š ---\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    MODEL_NAME = \"openai/whisper-small\"\n",
    "    LANGUAGE = \"zh\"\n",
    "    TASK = \"transcribe\"\n",
    "    OUTPUT_DIR = \"./whisper-small-zh-finetune-final\"\n",
    "\n",
    "    # --- è¼‰å…¥ Processor å’Œæ¨¡å‹ ---\n",
    "    print(\"--- æ­¥é©Ÿ 1/4: è¼‰å…¥ Processor å’Œæ¨¡å‹ ---\")\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # --- å»ºç«‹åŸå§‹è³‡æ–™é›† ---\n",
    "    class AudioDatasetProcessor:\n",
    "        def __init__(self, file_path: str, target_sampling_rate: int = 16000):\n",
    "            self.file_path = file_path\n",
    "            self.target_sampling_rate = target_sampling_rate\n",
    "        def create_dataset(self) -> Dataset:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "            dataset = Dataset.from_pandas(full_data)\n",
    "            dataset = dataset.cast_column(\"file\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "            dataset = dataset.rename_column(\"file\", \"audio\")\n",
    "            return dataset\n",
    "            \n",
    "    print(\"\\n--- æ­¥é©Ÿ 2/4: å»ºç«‹åŸå§‹è³‡æ–™é›†ä¸¦è¨­å®šã€å³æ™‚è½‰æ›ã€---\")\n",
    "    audio_processor = AudioDatasetProcessor(file_path=CSV_PATH)\n",
    "    full_dataset = audio_processor.create_dataset()\n",
    "    common_voice = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    # ä½¿ç”¨ .with_transform() ç¢ºä¿è¨˜æ†¶é«”ç©©å®šï¼Œè¨“ç·´èƒ½ç«‹åˆ»é–‹å§‹\n",
    "    prepare_fn = partial(prepare_dataset_batched, feature_extractor=processor.feature_extractor, tokenizer=processor.tokenizer)\n",
    "    vectorized_datasets = common_voice.with_transform(prepare_fn)\n",
    "    print(\"å³æ™‚è½‰æ›å·²è¨­å®šã€‚\")\n",
    "\n",
    "    # --- å»ºç«‹è¨“ç·´å…ƒä»¶ ---\n",
    "    print(\"\\n--- æ­¥é©Ÿ 3/4: å»ºç«‹è¨“ç·´å…ƒä»¶ (æœ€çµ‚ç©©å®šé‹è¡Œç‰ˆ) ---\")\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)\n",
    "\n",
    "    # [æœ€çµ‚ä¿®æ­£]\n",
    "    # æ—¢ç„¶ dataloader_num_workers=0 æ˜¯å”¯ä¸€èƒ½é¿å…æ­»é–çš„æ–¹å¼ï¼Œ\n",
    "    # æˆ‘å€‘å¿…é ˆå°‡è¨“ç·´åƒæ•¸èª¿æ•´åˆ°èˆ‡ä¹‹åŒ¹é…çš„ä¿å®ˆæ°´å¹³ï¼Œä»¥é¿å… OOM éŒ¯èª¤ã€‚\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        \n",
    "        # 1. å¤§å¹…é™ä½æ‰¹æ¬¡å¤§å°ï¼Œé€™æ˜¯é¿å… OOM çš„æ ¸å¿ƒ\n",
    "        per_device_train_batch_size=4,   # å¾ 32 æˆ– 16 å¤§å¹…é™è‡³ 4ï¼Œé€™æ˜¯ä¸€å€‹æ¥µåº¦å®‰å…¨çš„å€¼\n",
    "        per_device_eval_batch_size=4,    # é©—è­‰æ‰¹æ¬¡ä¹Ÿä½¿ç”¨åŒæ¨£çš„å®‰å…¨å€¼\n",
    "        \n",
    "        # 2. é©åº¦ä½¿ç”¨æ¢¯åº¦ç´¯ç©ï¼Œä»¥ç©©å®šè¨“ç·´\n",
    "        # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ç‚º 4 * 4 = 16ï¼Œé€™æ˜¯ä¸€å€‹ä¸éŒ¯çš„å¹³è¡¡é»\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        # 3. ç¦ç”¨å¤šæ ¸å¿ƒè™•ç†ï¼Œé€™æ˜¯ç¢ºä¿ç¨‹å¼ä¸è¢«æ›èµ·çš„é—œéµ\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # --- å…¶ä»–åƒæ•¸ç¶­æŒä¸è®Š ---\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=5000,\n",
    "        gradient_checkpointing=False,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        save_steps=1000,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=vectorized_datasets[\"train\"],\n",
    "        eval_dataset=vectorized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    # --- é–‹å§‹è¨“ç·´ ---\n",
    "    print(\"\\n--- æ­¥é©Ÿ 4/4: é–‹å§‹æ¨¡å‹å¾®èª¿è¨“ç·´ (ä½¿ç”¨æœ€çµ‚ç©©å®šé«˜æ•ˆæ¨¡å¼) ---\")\n",
    "    # ä¸å¸¶åƒæ•¸çš„ .train() æœƒè‡ªå‹•è™•ç†æ–·é»çºŒç·´ï¼Œæ˜¯æœ€ç©©å¥çš„åšæ³•ã€‚\n",
    "    trainer.train() \n",
    "    print(\"\\n*** è¨“ç·´å®Œæˆ ***\")\n",
    "    \n",
    "    # --- å„²å­˜æœ€çµ‚æ¨¡å‹ ---\n",
    "    print(\"\\n--- æ­£åœ¨å„²å­˜æœ€çµ‚çš„æœ€ä½³æ¨¡å‹ ---\")\n",
    "    final_model_path = training_args.output_dir\n",
    "    trainer.save_model(final_model_path)\n",
    "    processor.save_pretrained(final_model_path)\n",
    "    print(f\"\\næœ€çµ‚æ¨¡å‹å·²å„²å­˜è‡³ï¼š{final_model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # ç¢ºä¿æ‚¨å·²åœ¨çµ‚ç«¯æ©Ÿä½¿ç”¨ `huggingface-cli login` ç™»å…¥\n",
    "    # åŸ·è¡Œå‰å»ºè­°é‡æ–°å•Ÿå‹•æ‚¨çš„é›»è…¦ï¼Œç¢ºä¿ç³»çµ±è™•æ–¼ä¹¾æ·¨ç‹€æ…‹\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcefe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch GPU ç’°å¢ƒæª¢æ¸¬å ±å‘Š ---\n",
      "\n",
      "[âœ… æˆåŠŸ] `torch.cuda.is_available()` å›å ±ï¼šTrue\n",
      "æ‚¨çš„ç’°å¢ƒå·²æº–å‚™å¥½é€²è¡Œ GPU åŠ é€Ÿè¨“ç·´ï¼\n",
      "\n",
      "[â„¹ï¸ è³‡è¨Š] åµæ¸¬åˆ°çš„å¯ç”¨ GPU æ•¸é‡ï¼š1 å¼µ\n",
      "[â„¹ï¸ è³‡è¨Š] ç›®å‰ PyTorch å°‡ä½¿ç”¨çš„ GPUï¼šNVIDIA GeForce RTX 3060 Ti (cuda:0)\n",
      "\n",
      "--- æª¢æ¸¬å ±å‘ŠçµæŸ ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    ä¸€å€‹ç¨ç«‹çš„å‡½å¼ï¼Œç”¨æ–¼æª¢æŸ¥ PyTorch çš„ GPU (CUDA) ç’°å¢ƒæ˜¯å¦è¨­å®šæ­£ç¢ºã€‚\n",
    "    \"\"\"\n",
    "    print(\"--- PyTorch GPU ç’°å¢ƒæª¢æ¸¬å ±å‘Š ---\")\n",
    "    \n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    if is_cuda_available:\n",
    "        print(\"\\n[âœ… æˆåŠŸ] `torch.cuda.is_available()` å›å ±ï¼šTrue\")\n",
    "        print(\"æ‚¨çš„ç’°å¢ƒå·²æº–å‚™å¥½é€²è¡Œ GPU åŠ é€Ÿè¨“ç·´ï¼\")\n",
    "        \n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n[â„¹ï¸ è³‡è¨Š] åµæ¸¬åˆ°çš„å¯ç”¨ GPU æ•¸é‡ï¼š{gpu_count} å¼µ\")\n",
    "        \n",
    "        current_device_id = torch.cuda.current_device()\n",
    "        current_device_name = torch.cuda.get_device_name(current_device_id)\n",
    "        print(f\"[â„¹ï¸ è³‡è¨Š] ç›®å‰ PyTorch å°‡ä½¿ç”¨çš„ GPUï¼š{current_device_name} (cuda:{current_device_id})\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n[âŒ å¤±æ•—] `torch.cuda.is_available()` å›å ±ï¼šFalse\")\n",
    "        print(\"æ‚¨çš„ç’°å¢ƒç›®å‰ç„¡æ³•ä½¿ç”¨ GPUï¼Œè¨“ç·´å°‡æœƒç”± CPU åŸ·è¡Œï¼Œé€Ÿåº¦æœƒéå¸¸æ…¢ã€‚\")\n",
    "        \n",
    "    print(\"\\n--- æª¢æ¸¬å ±å‘ŠçµæŸ ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1693e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "c:\\Users\\User\\audio_model\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)\n",
    "import sys; print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"--- Python ç³»çµ±è·¯å¾‘ (sys.path) æœæŸ¥å ±å‘Š ---\")\n",
    "print(\"Python æœƒä¾ç…§ä»¥ä¸‹é †åºå°‹æ‰¾å‡½å¼åº«ï¼š\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "conflict_found = False\n",
    "# éæ­·æ‰€æœ‰ Python æœƒå°‹æ‰¾çš„è·¯å¾‘\n",
    "for path in sys.path:\n",
    "    # æˆ‘å€‘åªé—œå¿ƒçœŸå¯¦å­˜åœ¨çš„è³‡æ–™å¤¾\n",
    "    if os.path.isdir(path):\n",
    "        # å°‹æ‰¾ä»»ä½•å¯èƒ½é€ æˆè¡çªçš„æª”æ¡ˆ\n",
    "        potential_conflict_file = os.path.join(path, \"datasets.py\")\n",
    "        if os.path.exists(potential_conflict_file):\n",
    "            print(f\"[ğŸš¨ é‡å¤§ç™¼ç¾!] åœ¨ä»¥ä¸‹è·¯å¾‘ä¸­æ‰¾åˆ°äº†åç‚º datasets.py çš„è¡çªæª”æ¡ˆï¼š\")\n",
    "            print(f\"==> {potential_conflict_file}\")\n",
    "            print(\"é€™å°±æ˜¯é€ æˆæ‚¨éŒ¯èª¤çš„æ ¹æœ¬åŸå› ï¼è«‹ç«‹åˆ»å°‡æ­¤æª”æ¡ˆé‡æ–°å‘½åæˆ–åˆªé™¤ã€‚\")\n",
    "            conflict_found = True\n",
    "            break # æ‰¾åˆ°ä¸€å€‹å°±å¤ äº†\n",
    "\n",
    "if not conflict_found:\n",
    "    print(\"[âœ… æ­£å¸¸] åœ¨æ‰€æœ‰ Python æœå°‹è·¯å¾‘ä¸­ï¼Œæœªç™¼ç¾åç‚º datasets.py çš„è¡çªæª”æ¡ˆã€‚\")\n",
    "    print(\"é€™è¡¨ç¤ºå•é¡Œå¯èƒ½å‡ºåœ¨å‡½å¼åº«å®‰è£æœ¬èº«ã€‚è«‹ç¹¼çºŒåŸ·è¡Œä¸‹ä¸€æ­¥ã€‚\")\n",
    "\n",
    "print(\"\\n--- æ­£åœ¨å®šä½ `datasets` å‡½å¼åº«çš„å¯¦éš›ä½ç½® ---\")\n",
    "try:\n",
    "    import datasets\n",
    "    # __file__ å±¬æ€§æœƒå‘Šè¨´æˆ‘å€‘é€™å€‹æ¨¡çµ„æ˜¯å¾å“ªå€‹æª”æ¡ˆè¼‰å…¥çš„\n",
    "    print(f\"[â„¹ï¸ è³‡è¨Š] ç•¶æ‚¨ `import datasets` æ™‚ï¼ŒPython å¯¦éš›è¼‰å…¥çš„æª”æ¡ˆæ˜¯ï¼š\")\n",
    "    print(f\"==> {datasets.__file__}\")\n",
    "    print(\"è«‹æª¢æŸ¥é€™å€‹è·¯å¾‘æ˜¯å¦åœ¨æ‚¨çš„ .venv è™›æ“¬ç’°å¢ƒçš„ site-packages ä¸­ã€‚å¦‚æœä¸æ˜¯ï¼Œä»£è¡¨æ‚¨çš„ç’°å¢ƒè¨­å®šæœ‰èª¤ã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"[âŒ éŒ¯èª¤] ç”šè‡³ç„¡æ³•æˆåŠŸ `import datasets`ï¼Œé€™å¼·çƒˆæš—ç¤ºæ‚¨çš„å®‰è£å·²æå£ã€‚\")\n",
    "    print(f\"éŒ¯èª¤è¨Šæ¯: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0247ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    ä¸€å€‹ç¨ç«‹çš„å‡½å¼ï¼Œç”¨æ–¼æª¢æŸ¥ PyTorch çš„ GPU (CUDA) ç’°å¢ƒæ˜¯å¦è¨­å®šæ­£ç¢ºã€‚\n",
    "    \"\"\"\n",
    "    print(\"--- PyTorch GPU ç’°å¢ƒæª¢æ¸¬å ±å‘Š ---\")\n",
    "    \n",
    "    # 1. æ ¸å¿ƒæª¢æ¸¬ï¼štorch.cuda.is_available()\n",
    "    #    é€™æ˜¯åˆ¤æ–· PyTorch èƒ½å¦ä½¿ç”¨ GPU çš„é»ƒé‡‘æ¨™æº–ã€‚\n",
    "    #    å®ƒæœƒæª¢æŸ¥ NVIDIA é©…å‹•ã€CUDA å·¥å…·åŒ…æ˜¯å¦éƒ½å·²å®‰è£ä¸”è¢« PyTorch æ­£ç¢ºåµæ¸¬ã€‚\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    if is_cuda_available:\n",
    "        print(\"\\n[âœ… æˆåŠŸ] `torch.cuda.is_available()` å›å ±ï¼šTrue\")\n",
    "        print(\"æ­å–œï¼æ‚¨çš„ PyTorch ç’°å¢ƒå·²æˆåŠŸåµæ¸¬åˆ°ä¸¦å¯ä»¥ä½¿ç”¨ GPU (CUDA)ã€‚\")\n",
    "        \n",
    "        # 2. ç²å– GPU æ•¸é‡\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n[â„¹ï¸ è³‡è¨Š] åµæ¸¬åˆ°çš„å¯ç”¨ GPU æ•¸é‡ï¼š{gpu_count} å¼µ\")\n",
    "        \n",
    "        # 3. ç²å–ç•¶å‰ä½¿ç”¨çš„ GPU è³‡è¨Š\n",
    "        #    PyTorch é è¨­ä½¿ç”¨ç¬¬ 0 å¼µ GPUã€‚\n",
    "        current_device_id = torch.cuda.current_device()\n",
    "        current_device_name = torch.cuda.get_device_name(current_device_id)\n",
    "        print(f\"[â„¹ï¸ è³‡è¨Š] ç›®å‰ PyTorch é è¨­ä½¿ç”¨çš„ GPU ç·¨è™Ÿï¼š{current_device_id}\")\n",
    "        print(f\"[â„¹ï¸ è³‡è¨Š] GPU å‹è™Ÿï¼š{current_device_name}\")\n",
    "        \n",
    "        # 4. å¯¦æˆ°é©—è­‰ï¼šå˜—è©¦å°‡å¼µé‡ (Tensor) ç§»è‡³ GPU ä¸¦é€²è¡Œè¨ˆç®—\n",
    "        print(\"\\n--- æ­£åœ¨é€²è¡Œå¯¦æˆ°é©—è­‰ ---\")\n",
    "        try:\n",
    "            # å‰µå»ºä¸€å€‹å¼µé‡ä¸¦æ˜ç¢ºæŒ‡å®šè¦ä½¿ç”¨çš„ CUDA è¨­å‚™\n",
    "            device = torch.device(\"cuda\")\n",
    "            tensor_cpu = torch.randn(3, 4)\n",
    "            print(f\"æ­¥é©Ÿ 1: åœ¨ CPU ä¸Šå»ºç«‹ä¸€å€‹å¼µé‡ï¼Œä½ç½®ï¼š{tensor_cpu.device}\")\n",
    "            \n",
    "            # ä½¿ç”¨ .to(device) å°‡å¼µé‡ç§»è‡³ GPU\n",
    "            tensor_gpu = tensor_cpu.to(device)\n",
    "            print(f\"æ­¥é©Ÿ 2: å·²æˆåŠŸå°‡å¼µé‡è¤‡è£½åˆ° GPUï¼Œæ–°ä½ç½®ï¼š{tensor_gpu.device}\")\n",
    "            \n",
    "            # åœ¨ GPU ä¸ŠåŸ·è¡Œä¸€å€‹ç°¡å–®çš„çŸ©é™£ä¹˜æ³•\n",
    "            result_gpu = torch.matmul(tensor_gpu.T, tensor_gpu)\n",
    "            print(\"æ­¥é©Ÿ 3: å·²åœ¨ GPU ä¸ŠæˆåŠŸå®ŒæˆçŸ©é™£ä¹˜æ³•é‹ç®—ã€‚\")\n",
    "            \n",
    "            print(\"\\n[âœ… çµè«–] GPU é©—è­‰æˆåŠŸï¼æ‚¨çš„ç’°å¢ƒå·²æº–å‚™å¥½é€²è¡Œé«˜æ•ˆçš„æ·±åº¦å­¸ç¿’è¨“ç·´ã€‚\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"\\n[âŒ éŒ¯èª¤] åœ¨å¯¦æˆ°é©—è­‰éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œé€™ä¸æ‡‰è©²ç™¼ç”Ÿã€‚è«‹æª¢æŸ¥æ‚¨çš„é©…å‹•ç¨‹å¼ã€‚\")\n",
    "            print(f\"éŒ¯èª¤è¨Šæ¯ï¼š{e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n[âŒ å¤±æ•—] `torch.cuda.is_available()` å›å ±ï¼šFalse\")\n",
    "        print(\"å¾ˆæŠ±æ­‰ï¼Œæ‚¨çš„ PyTorch ç’°å¢ƒç›®å‰ç„¡æ³•ä½¿ç”¨ GPUã€‚\")\n",
    "        print(\"å¯èƒ½çš„åŸå› åŒ…æ‹¬ï¼š\")\n",
    "        print(\"  1. æ‚¨çš„é›»è…¦æ²’æœ‰å®‰è£ NVIDIA çš„ GPUã€‚\")\n",
    "        print(\"  2. æ‚¨å°šæœªå®‰è£ NVIDIA å®˜æ–¹é©…å‹•ç¨‹å¼ã€‚\")\n",
    "        print(\"  3. æ‚¨å®‰è£çš„ PyTorch ç‰ˆæœ¬æ˜¯ CPU-only ç‰ˆæœ¬ã€‚\")\n",
    "        print(\"  4. CUDA å·¥å…·åŒ…èˆ‡ PyTorch æˆ–é©…å‹•ç¨‹å¼ç‰ˆæœ¬ä¸ç›¸å®¹ã€‚\")\n",
    "        print(\"\\nå»ºè­°æ“ä½œï¼šè«‹ç¢ºèªä»¥ä¸Šå¹¾é»ï¼Œä¸¦è€ƒæ…®åœ¨æœ‰ GPU çš„ç’°å¢ƒï¼ˆå¦‚ Google Colabï¼‰ä¸­åŸ·è¡Œæ‚¨çš„è¨“ç·´ç¨‹å¼ç¢¼ã€‚\")\n",
    "        \n",
    "    print(\"\\n--- æª¢æ¸¬å ±å‘ŠçµæŸ ---\")\n",
    "\n",
    "# åŸ·è¡Œæª¢æ¸¬å‡½å¼\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f07652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "\n",
    "# å‡è¨­ä½ çš„è³‡æ–™é›†æ˜¯ CSV æ ¼å¼\n",
    "data = pd.read_csv('output/final_audio_paths.csv')\n",
    "\n",
    "# å°‡è³‡æ–™è½‰æ›ç‚º Hugging Face Dataset æ ¼å¼\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"test\": Dataset.from_pandas(data.sample(frac=0.2))  # éš¨æ©Ÿå– 20% ä½œç‚ºæ¸¬è©¦é›†\n",
    "})\n",
    "\n",
    "# å®šç¾© load_audio å‡½æ•¸\n",
    "def load_audio(file_path):\n",
    "    \"\"\"åŠ è¼‰éŸ³é »æ–‡ä»¶ä¸¦è¿”å›éŸ³é »æ•¸çµ„å’Œå–æ¨£ç‡\"\"\"\n",
    "    audio_array, sampling_rate = librosa.load(file_path, sr=None)  # sr=None ä¿æŒåŸå§‹å–æ¨£ç‡\n",
    "    return audio_array, sampling_rate\n",
    "\n",
    "# åˆå§‹åŒ– feature_extractor å’Œ tokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"zh\", task=\"transcribe\")\n",
    "\n",
    "# å®šç¾©æ•¸æ“šé›†è™•ç†å‡½æ•¸\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"file\"]  # å‡è¨­é€™æ˜¯éŸ³é »æ–‡ä»¶çš„è·¯å¾‘\n",
    "    # è®€å–éŸ³é »æ–‡ä»¶ä¸¦è½‰æ›\n",
    "    audio_array, sampling_rate = load_audio(audio)  # ä½¿ç”¨è‡ªå®šç¾©çš„ load_audio å‡½æ•¸\n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# ä½¿ç”¨ map æ–¹æ³•è™•ç†æ•¸æ“šé›†\n",
    "# dataset[\"train\"] = dataset[\"train\"].map(prepare_dataset, remove_columns=[\"transcription\", \"file\"], num_proc=4)\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"transcription\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=16,  # æ¸›å°‘æ‰¹æ¬¡å¤§å°\n",
    "    keep_in_memory=False,  # è¨­ç½®ç‚º False\n",
    "    num_proc=4  # ä½¿ç”¨ 4 å€‹é€²ç¨‹\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"transcription\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=16,  # æ¸›å°‘æ‰¹æ¬¡å¤§å°\n",
    "    keep_in_memory=False,  # è¨­ç½®ç‚º False\n",
    "    num_proc=4  # ä½¿ç”¨ 4 å€‹é€²ç¨‹\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
