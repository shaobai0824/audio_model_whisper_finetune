{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb24ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset,DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# 假設你的資料集是 CSV 格式\n",
    "data = pd.read_csv('output/final_audio_paths.csv')\n",
    "\n",
    "# 將資料轉換為 Hugging Face Dataset 格式\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"test\": Dataset.from_pandas(data.sample(frac=0.2))  # 隨機取 20% 作為測試集\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f291f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any\n",
    "\n",
    "class AudioDatasetProcessor_V3:\n",
    "    \"\"\"\n",
    "    處理音訊 CSV 的最終版本類別。\n",
    "    - 讀取 'path' 與 'transcription' 欄位。\n",
    "    - 將 'transcription' 重新命名為 'sentence'。\n",
    "    - 產生與目標格式完全匹配的 DatasetDict。\n",
    "\n",
    "    屬性:\n",
    "        file_path (str): 輸入的 CSV 檔案路徑。\n",
    "        target_sampling_rate (int): 目標音訊取樣率。\n",
    "        test_size (float): 用於劃分測試集的資料比例。\n",
    "        random_state (int): 隨機種子，確保可重複的資料切分。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, target_sampling_rate: int = 16000, test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"初始化 AudioDatasetProcessor_V3\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_single_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        \"\"\"\n",
    "        將單一 DataFrame 轉換為具有指定取樣率 Audio 特徵的 Dataset。\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 包含 'audio' 和 'sentence' 欄位的 DataFrame。\n",
    "\n",
    "        Returns:\n",
    "            Dataset: 已處理的 Dataset 物件。\n",
    "        \"\"\"\n",
    "        # 複製 'path' 欄位到 'audio'，此時 df 應包含 'path' 和 'sentence'\n",
    "        temp_df = df.copy()\n",
    "        temp_df['audio'] = temp_df['file']\n",
    "\n",
    "        dataset = Dataset.from_pandas(temp_df)\n",
    "        \n",
    "        # 轉換 audio 欄位型別並移除多餘的 path 欄位\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        執行完整的資料處理流程，包含欄位重新命名。\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: 包含 'train' 和 'test' splits 的最終物件。\n",
    "        \"\"\"\n",
    "        # 讀取 CSV\n",
    "        try:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"錯誤：找不到檔案 {self.file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # 關鍵步驟 1: 指定來源需要的欄位是 'path' 和 'transcription'\n",
    "        required_columns = ['transcription', 'file']\n",
    "        if not all(col in full_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV 檔案缺少必要欄位。需要: {required_columns}\")\n",
    "        \n",
    "        filtered_data = full_data[required_columns]\n",
    "\n",
    "        # 使用處理後的 DataFrame 進行資料切分\n",
    "        train_df, test_df = train_test_split(\n",
    "             filtered_data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # 分別為訓練集和測試集準備 Dataset\n",
    "        train_dataset = self._prepare_single_dataset(train_df)\n",
    "        test_dataset = self._prepare_single_dataset(test_df)\n",
    "\n",
    "        final_dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        })\n",
    "\n",
    "        return final_dataset_dict\n",
    "\n",
    "# --- 使用範例 ---\n",
    "if __name__ == '__main__':\n",
    "    # 假設您的 CSV 檔案名為 'final_audio_paths.csv'\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    \n",
    "    # 實例化最終版本的處理器\n",
    "    # 明確指定取樣率為 16000 Hz\n",
    "    processor_v3 = AudioDatasetProcessor_V3(file_path=CSV_PATH, target_sampling_rate=16000)\n",
    "    final_dataset = processor_v3.create_dataset_dict()\n",
    "\n",
    "    if final_dataset:\n",
    "        print(\"--- 最終版資料集結構 ---\")\n",
    "        print(final_dataset)\n",
    "        \n",
    "        print(\"\\n--- Train Split Features ---\")\n",
    "        print(final_dataset['train'].features)\n",
    "\n",
    "        print(\"\\n--- 抽查第一筆訓練資料 (驗證最終格式) ---\")\n",
    "        print(final_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66819195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "class AudioDatasetProcessor_V3:\n",
    "    \"\"\"\n",
    "    處理音訊 CSV 的最終版本類別。\n",
    "    - 讀取 'path' 與 'transcription' 欄位。\n",
    "    - 將 'transcription' 重新命名為 'sentence'。\n",
    "    - 產生與目標格式完全匹配的 DatasetDict。\n",
    "\n",
    "    屬性:\n",
    "        file_path (str): 輸入的 CSV 檔案路徑。\n",
    "        target_sampling_rate (int): 目標音訊取樣率。\n",
    "        test_size (float): 用於劃分測試集的資料比例。\n",
    "        random_state (int): 隨機種子，確保可重複的資料切分。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, target_sampling_rate: int = 16000, test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"初始化 AudioDatasetProcessor_V3\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_single_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        \"\"\"\n",
    "        將單一 DataFrame 轉換為具有指定取樣率 Audio 特徵的 Dataset。\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 包含 'audio' 和 'sentence' 欄位的 DataFrame。\n",
    "\n",
    "        Returns:\n",
    "            Dataset: 已處理的 Dataset 物件。\n",
    "        \"\"\"\n",
    "        # 複製 'path' 欄位到 'audio'，此時 df 應包含 'path' 和 'sentence'\n",
    "        temp_df = df.copy()\n",
    "        temp_df['audio'] = temp_df['file']\n",
    "\n",
    "        dataset = Dataset.from_pandas(temp_df)\n",
    "        \n",
    "        # 轉換 audio 欄位型別並移除多餘的 path 欄位\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        執行完整的資料處理流程，包含欄位重新命名。\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: 包含 'train' 和 'test' splits 的最終物件。\n",
    "        \"\"\"\n",
    "        # 讀取 CSV\n",
    "        try:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"錯誤：找不到檔案 {self.file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # 關鍵步驟 1: 指定來源需要的欄位是 'path' 和 'transcription'\n",
    "        required_columns = ['transcription', 'file']\n",
    "        if not all(col in full_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV 檔案缺少必要欄位。需要: {required_columns}\")\n",
    "        \n",
    "        filtered_data = full_data[required_columns]\n",
    "\n",
    "        # 使用處理後的 DataFrame 進行資料切分\n",
    "        train_df, test_df = train_test_split(\n",
    "             filtered_data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # 分別為訓練集和測試集準備 Dataset\n",
    "        train_dataset = self._prepare_single_dataset(train_df)\n",
    "        test_dataset = self._prepare_single_dataset(test_df)\n",
    "\n",
    "        final_dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        })\n",
    "\n",
    "        return final_dataset_dict\n",
    "\n",
    "# --- 使用範例 ---\n",
    "if __name__ == '__main__':\n",
    "    # 假設您的 CSV 檔案名為 'final_audio_paths.csv'\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    \n",
    "    # 實例化最終版本的處理器\n",
    "    # 明確指定取樣率為 16000 Hz\n",
    "    processor_v3 = AudioDatasetProcessor_V3(file_path=CSV_PATH, target_sampling_rate=16000)\n",
    "    final_dataset = processor_v3.create_dataset_dict()\n",
    "\n",
    "    if final_dataset:\n",
    "        print(\"--- 最終版資料集結構 ---\")\n",
    "        print(final_dataset)\n",
    "        \n",
    "        print(\"\\n--- Train Split Features ---\")\n",
    "        print(final_dataset['train'].features)\n",
    "\n",
    "        print(\"\\n--- 抽查第一筆訓練資料 (驗證最終格式) ---\")\n",
    "        print(final_dataset['train'][0])\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"zh\", task=\"transcribe\")\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = final_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "input_str = final_dataset[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "final_dataset = final_dataset.map(prepare_dataset, remove_columns=final_dataset.column_names[\"train\"], num_proc=4)\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# print(f\"Input:                 {input_str}\")\n",
    "# print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "# print(f\"Decoded w/out special: {decoded_str}\")\n",
    "# print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b48cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafa7399599449fd96fa757c2289d767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6509c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步驟 1/4: 載入 Processor 和模型 ---\n",
      "\n",
      "--- 步驟 2/4: 建立原始資料集並設定『即時轉換』---\n",
      "即時轉換已設定。\n",
      "\n",
      "--- 步驟 3/4: 建立訓練元件 (最終穩定運行版) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_9332\\4225368149.py:153: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步驟 4/4: 開始模型微調訓練 (使用最終穩定高效模式) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='341' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 341/5000 32:48 < 7:30:57, 0.17 it/s, Epoch 0.08/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m最終模型已儲存至：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# 確保您已在終端機使用 `huggingface-cli login` 登入\u001b[39;00m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# 執行前建議重新啟動您的電腦，確保系統處於乾淨狀態\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 步驟 4/4: 開始模型微調訓練 (使用最終穩定高效模式) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# 不帶參數的 .train() 會自動處理斷點續練，是最穩健的做法。\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m*** 訓練完成 ***\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# --- 儲存最終模型 ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2197\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2195\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2196\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2197\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2204\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3749\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3749\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3751\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3753\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3754\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3755\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3836\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3834\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3835\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3836\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3838\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1339\u001b[39m, in \u001b[36mWhisperForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1335\u001b[39m         decoder_input_ids = shift_tokens_right(\n\u001b[32m   1336\u001b[39m             labels, \u001b[38;5;28mself\u001b[39m.config.pad_token_id, \u001b[38;5;28mself\u001b[39m.config.decoder_start_token_id\n\u001b[32m   1337\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1339\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m lm_logits = \u001b[38;5;28mself\u001b[39m.proj_out(outputs[\u001b[32m0\u001b[39m])\n\u001b[32m   1359\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:1174\u001b[39m, in \u001b[36mWhisperModel.forward\u001b[39m\u001b[34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m   1167\u001b[39m     encoder_outputs = BaseModelOutput(\n\u001b[32m   1168\u001b[39m         last_hidden_state=encoder_outputs[\u001b[32m0\u001b[39m],\n\u001b[32m   1169\u001b[39m         hidden_states=encoder_outputs[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1170\u001b[39m         attentions=encoder_outputs[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) > \u001b[32m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1171\u001b[39m     )\n\u001b[32m   1173\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m decoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m   1191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs + encoder_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:960\u001b[39m, in \u001b[36mWhisperDecoder.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, encoder_hidden_states, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    957\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability < \u001b[38;5;28mself\u001b[39m.layerdrop:\n\u001b[32m    958\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:502\u001b[39m, in \u001b[36mWhisperDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache, cache_position)\u001b[39m\n\u001b[32m    483\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[33;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m \u001b[33;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    501\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn_layer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    505\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    506\u001b[39m     hidden_states=hidden_states,\n\u001b[32m    507\u001b[39m     past_key_value=past_key_value,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m     cache_position=cache_position,\n\u001b[32m    512\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2900\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2891\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2892\u001b[39m         layer_norm,\n\u001b[32m   2893\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2898\u001b[39m         eps=eps,\n\u001b[32m   2899\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2900\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2901\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 檔案：train_final.py\n",
    "# 描述：一個完整、高效、穩健的 Whisper 模型微調流程的最終版本。\n",
    "# 核心策略：\n",
    "# 1. 即時轉換 (.with_transform)：徹底解決記憶體不足與預處理過久的問題。\n",
    "# 2. 背景預取 (dataloader_num_workers)：解決 CPU 與 I/O 瓶頸，最大化 GPU 使用率。\n",
    "# 3. 全域定義 (Global Scope)：解決多核心處理時的 pickling 錯誤。\n",
    "# 4. 智慧續練 (Smart Resuming)：自動從上次的檢查點恢復訓練。\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Union\n",
    "from dataclasses import dataclass\n",
    "import evaluate\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "# --- Hugging Face 相關導入 ---\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "\n",
    "# ==============================================================================\n",
    "# 步驟 1: 將所有輔助類別與函式定義在「全域範圍」\n",
    "# 這是為了確保在使用 dataloader_num_workers > 0 時，背景程序可以成功序列化 (pickle) 它們。\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"處理語音到序列資料的 Data Collator，負責將樣本整理成批次並進行填充。\"\"\"\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset_batched(batch, feature_extractor, tokenizer):\n",
    "    \"\"\"將一批音訊和文本資料『即時』轉換為模型輸入格式。\"\"\"\n",
    "    audio_list = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        [x[\"array\"] for x in audio_list], \n",
    "        sampling_rate=audio_list[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"], max_length=448, truncation=True).input_ids\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(pred, tokenizer):\n",
    "    \"\"\"在評估階段，計算並回傳 WER 指標。\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# ==============================================================================\n",
    "# 步驟 2: 主執行流程\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    # --- 參數設定 ---\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    MODEL_NAME = \"openai/whisper-small\"\n",
    "    LANGUAGE = \"zh\"\n",
    "    TASK = \"transcribe\"\n",
    "    OUTPUT_DIR = \"./whisper-small-zh-finetune-final\"\n",
    "\n",
    "    # --- 載入 Processor 和模型 ---\n",
    "    print(\"--- 步驟 1/4: 載入 Processor 和模型 ---\")\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # --- 建立原始資料集 ---\n",
    "    class AudioDatasetProcessor:\n",
    "        def __init__(self, file_path: str, target_sampling_rate: int = 16000):\n",
    "            self.file_path = file_path\n",
    "            self.target_sampling_rate = target_sampling_rate\n",
    "        def create_dataset(self) -> Dataset:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "            dataset = Dataset.from_pandas(full_data)\n",
    "            dataset = dataset.cast_column(\"file\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "            dataset = dataset.rename_column(\"file\", \"audio\")\n",
    "            return dataset\n",
    "            \n",
    "    print(\"\\n--- 步驟 2/4: 建立原始資料集並設定『即時轉換』---\")\n",
    "    audio_processor = AudioDatasetProcessor(file_path=CSV_PATH)\n",
    "    full_dataset = audio_processor.create_dataset()\n",
    "    common_voice = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    # 使用 .with_transform() 確保記憶體穩定，訓練能立刻開始\n",
    "    prepare_fn = partial(prepare_dataset_batched, feature_extractor=processor.feature_extractor, tokenizer=processor.tokenizer)\n",
    "    vectorized_datasets = common_voice.with_transform(prepare_fn)\n",
    "    print(\"即時轉換已設定。\")\n",
    "\n",
    "    # --- 建立訓練元件 ---\n",
    "    print(\"\\n--- 步驟 3/4: 建立訓練元件 (最終穩定運行版) ---\")\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)\n",
    "\n",
    "    # [最終修正]\n",
    "    # 既然 dataloader_num_workers=0 是唯一能避免死鎖的方式，\n",
    "    # 我們必須將訓練參數調整到與之匹配的保守水平，以避免 OOM 錯誤。\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        \n",
    "        # 1. 大幅降低批次大小，這是避免 OOM 的核心\n",
    "        per_device_train_batch_size=4,   # 從 32 或 16 大幅降至 4，這是一個極度安全的值\n",
    "        per_device_eval_batch_size=4,    # 驗證批次也使用同樣的安全值\n",
    "        \n",
    "        # 2. 適度使用梯度累積，以穩定訓練\n",
    "        # 有效批次大小為 4 * 4 = 16，這是一個不錯的平衡點\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        # 3. 禁用多核心處理，這是確保程式不被掛起的關鍵\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # --- 其他參數維持不變 ---\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=5000,\n",
    "        gradient_checkpointing=False,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        save_steps=1000,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=vectorized_datasets[\"train\"],\n",
    "        eval_dataset=vectorized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    # --- 開始訓練 ---\n",
    "    print(\"\\n--- 步驟 4/4: 開始模型微調訓練 (使用最終穩定高效模式) ---\")\n",
    "    # 不帶參數的 .train() 會自動處理斷點續練，是最穩健的做法。\n",
    "    trainer.train() \n",
    "    print(\"\\n*** 訓練完成 ***\")\n",
    "    \n",
    "    # --- 儲存最終模型 ---\n",
    "    print(\"\\n--- 正在儲存最終的最佳模型 ---\")\n",
    "    final_model_path = training_args.output_dir\n",
    "    trainer.save_model(final_model_path)\n",
    "    processor.save_pretrained(final_model_path)\n",
    "    print(f\"\\n最終模型已儲存至：{final_model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 確保您已在終端機使用 `huggingface-cli login` 登入\n",
    "    # 執行前建議重新啟動您的電腦，確保系統處於乾淨狀態\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcefe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch GPU 環境檢測報告 ---\n",
      "\n",
      "[✅ 成功] `torch.cuda.is_available()` 回報：True\n",
      "您的環境已準備好進行 GPU 加速訓練！\n",
      "\n",
      "[ℹ️ 資訊] 偵測到的可用 GPU 數量：1 張\n",
      "[ℹ️ 資訊] 目前 PyTorch 將使用的 GPU：NVIDIA GeForce RTX 3060 Ti (cuda:0)\n",
      "\n",
      "--- 檢測報告結束 ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    一個獨立的函式，用於檢查 PyTorch 的 GPU (CUDA) 環境是否設定正確。\n",
    "    \"\"\"\n",
    "    print(\"--- PyTorch GPU 環境檢測報告 ---\")\n",
    "    \n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    if is_cuda_available:\n",
    "        print(\"\\n[✅ 成功] `torch.cuda.is_available()` 回報：True\")\n",
    "        print(\"您的環境已準備好進行 GPU 加速訓練！\")\n",
    "        \n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n[ℹ️ 資訊] 偵測到的可用 GPU 數量：{gpu_count} 張\")\n",
    "        \n",
    "        current_device_id = torch.cuda.current_device()\n",
    "        current_device_name = torch.cuda.get_device_name(current_device_id)\n",
    "        print(f\"[ℹ️ 資訊] 目前 PyTorch 將使用的 GPU：{current_device_name} (cuda:{current_device_id})\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n[❌ 失敗] `torch.cuda.is_available()` 回報：False\")\n",
    "        print(\"您的環境目前無法使用 GPU，訓練將會由 CPU 執行，速度會非常慢。\")\n",
    "        \n",
    "    print(\"\\n--- 檢測報告結束 ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1693e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; print(torch.__version__)\n",
    "import sys; print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"--- Python 系統路徑 (sys.path) 搜查報告 ---\")\n",
    "print(\"Python 會依照以下順序尋找函式庫：\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "conflict_found = False\n",
    "# 遍歷所有 Python 會尋找的路徑\n",
    "for path in sys.path:\n",
    "    # 我們只關心真實存在的資料夾\n",
    "    if os.path.isdir(path):\n",
    "        # 尋找任何可能造成衝突的檔案\n",
    "        potential_conflict_file = os.path.join(path, \"datasets.py\")\n",
    "        if os.path.exists(potential_conflict_file):\n",
    "            print(f\"[🚨 重大發現!] 在以下路徑中找到了名為 datasets.py 的衝突檔案：\")\n",
    "            print(f\"==> {potential_conflict_file}\")\n",
    "            print(\"這就是造成您錯誤的根本原因！請立刻將此檔案重新命名或刪除。\")\n",
    "            conflict_found = True\n",
    "            break # 找到一個就夠了\n",
    "\n",
    "if not conflict_found:\n",
    "    print(\"[✅ 正常] 在所有 Python 搜尋路徑中，未發現名為 datasets.py 的衝突檔案。\")\n",
    "    print(\"這表示問題可能出在函式庫安裝本身。請繼續執行下一步。\")\n",
    "\n",
    "print(\"\\n--- 正在定位 `datasets` 函式庫的實際位置 ---\")\n",
    "try:\n",
    "    import datasets\n",
    "    # __file__ 屬性會告訴我們這個模組是從哪個檔案載入的\n",
    "    print(f\"[ℹ️ 資訊] 當您 `import datasets` 時，Python 實際載入的檔案是：\")\n",
    "    print(f\"==> {datasets.__file__}\")\n",
    "    print(\"請檢查這個路徑是否在您的 .venv 虛擬環境的 site-packages 中。如果不是，代表您的環境設定有誤。\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ 錯誤] 甚至無法成功 `import datasets`，這強烈暗示您的安裝已損壞。\")\n",
    "    print(f\"錯誤訊息: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0247ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    一個獨立的函式，用於檢查 PyTorch 的 GPU (CUDA) 環境是否設定正確。\n",
    "    \"\"\"\n",
    "    print(\"--- PyTorch GPU 環境檢測報告 ---\")\n",
    "    \n",
    "    # 1. 核心檢測：torch.cuda.is_available()\n",
    "    #    這是判斷 PyTorch 能否使用 GPU 的黃金標準。\n",
    "    #    它會檢查 NVIDIA 驅動、CUDA 工具包是否都已安裝且被 PyTorch 正確偵測。\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    if is_cuda_available:\n",
    "        print(\"\\n[✅ 成功] `torch.cuda.is_available()` 回報：True\")\n",
    "        print(\"恭喜！您的 PyTorch 環境已成功偵測到並可以使用 GPU (CUDA)。\")\n",
    "        \n",
    "        # 2. 獲取 GPU 數量\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n[ℹ️ 資訊] 偵測到的可用 GPU 數量：{gpu_count} 張\")\n",
    "        \n",
    "        # 3. 獲取當前使用的 GPU 資訊\n",
    "        #    PyTorch 預設使用第 0 張 GPU。\n",
    "        current_device_id = torch.cuda.current_device()\n",
    "        current_device_name = torch.cuda.get_device_name(current_device_id)\n",
    "        print(f\"[ℹ️ 資訊] 目前 PyTorch 預設使用的 GPU 編號：{current_device_id}\")\n",
    "        print(f\"[ℹ️ 資訊] GPU 型號：{current_device_name}\")\n",
    "        \n",
    "        # 4. 實戰驗證：嘗試將張量 (Tensor) 移至 GPU 並進行計算\n",
    "        print(\"\\n--- 正在進行實戰驗證 ---\")\n",
    "        try:\n",
    "            # 創建一個張量並明確指定要使用的 CUDA 設備\n",
    "            device = torch.device(\"cuda\")\n",
    "            tensor_cpu = torch.randn(3, 4)\n",
    "            print(f\"步驟 1: 在 CPU 上建立一個張量，位置：{tensor_cpu.device}\")\n",
    "            \n",
    "            # 使用 .to(device) 將張量移至 GPU\n",
    "            tensor_gpu = tensor_cpu.to(device)\n",
    "            print(f\"步驟 2: 已成功將張量複製到 GPU，新位置：{tensor_gpu.device}\")\n",
    "            \n",
    "            # 在 GPU 上執行一個簡單的矩陣乘法\n",
    "            result_gpu = torch.matmul(tensor_gpu.T, tensor_gpu)\n",
    "            print(\"步驟 3: 已在 GPU 上成功完成矩陣乘法運算。\")\n",
    "            \n",
    "            print(\"\\n[✅ 結論] GPU 驗證成功！您的環境已準備好進行高效的深度學習訓練。\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"\\n[❌ 錯誤] 在實戰驗證過程中發生錯誤，這不應該發生。請檢查您的驅動程式。\")\n",
    "            print(f\"錯誤訊息：{e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n[❌ 失敗] `torch.cuda.is_available()` 回報：False\")\n",
    "        print(\"很抱歉，您的 PyTorch 環境目前無法使用 GPU。\")\n",
    "        print(\"可能的原因包括：\")\n",
    "        print(\"  1. 您的電腦沒有安裝 NVIDIA 的 GPU。\")\n",
    "        print(\"  2. 您尚未安裝 NVIDIA 官方驅動程式。\")\n",
    "        print(\"  3. 您安裝的 PyTorch 版本是 CPU-only 版本。\")\n",
    "        print(\"  4. CUDA 工具包與 PyTorch 或驅動程式版本不相容。\")\n",
    "        print(\"\\n建議操作：請確認以上幾點，並考慮在有 GPU 的環境（如 Google Colab）中執行您的訓練程式碼。\")\n",
    "        \n",
    "    print(\"\\n--- 檢測報告結束 ---\")\n",
    "\n",
    "# 執行檢測函式\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f07652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "\n",
    "# 假設你的資料集是 CSV 格式\n",
    "data = pd.read_csv('output/final_audio_paths.csv')\n",
    "\n",
    "# 將資料轉換為 Hugging Face Dataset 格式\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"test\": Dataset.from_pandas(data.sample(frac=0.2))  # 隨機取 20% 作為測試集\n",
    "})\n",
    "\n",
    "# 定義 load_audio 函數\n",
    "def load_audio(file_path):\n",
    "    \"\"\"加載音頻文件並返回音頻數組和取樣率\"\"\"\n",
    "    audio_array, sampling_rate = librosa.load(file_path, sr=None)  # sr=None 保持原始取樣率\n",
    "    return audio_array, sampling_rate\n",
    "\n",
    "# 初始化 feature_extractor 和 tokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"zh\", task=\"transcribe\")\n",
    "\n",
    "# 定義數據集處理函數\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"file\"]  # 假設這是音頻文件的路徑\n",
    "    # 讀取音頻文件並轉換\n",
    "    audio_array, sampling_rate = load_audio(audio)  # 使用自定義的 load_audio 函數\n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# 使用 map 方法處理數據集\n",
    "# dataset[\"train\"] = dataset[\"train\"].map(prepare_dataset, remove_columns=[\"transcription\", \"file\"], num_proc=4)\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"transcription\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=16,  # 減少批次大小\n",
    "    keep_in_memory=False,  # 設置為 False\n",
    "    num_proc=4  # 使用 4 個進程\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"transcription\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=16,  # 減少批次大小\n",
    "    keep_in_memory=False,  # 設置為 False\n",
    "    num_proc=4  # 使用 4 個進程\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
