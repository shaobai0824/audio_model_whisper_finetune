{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb24ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset,DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# 假設你的資料集是 CSV 格式\n",
    "data = pd.read_csv('output/final_audio_paths.csv')\n",
    "\n",
    "# 將資料轉換為 Hugging Face Dataset 格式\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"test\": Dataset.from_pandas(data.sample(frac=0.2))  # 隨機取 20% 作為測試集\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f291f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any\n",
    "\n",
    "class AudioDatasetProcessor_V3:\n",
    "    \"\"\"\n",
    "    處理音訊 CSV 的最終版本類別。\n",
    "    - 讀取 'path' 與 'transcription' 欄位。\n",
    "    - 將 'transcription' 重新命名為 'sentence'。\n",
    "    - 產生與目標格式完全匹配的 DatasetDict。\n",
    "\n",
    "    屬性:\n",
    "        file_path (str): 輸入的 CSV 檔案路徑。\n",
    "        target_sampling_rate (int): 目標音訊取樣率。\n",
    "        test_size (float): 用於劃分測試集的資料比例。\n",
    "        random_state (int): 隨機種子，確保可重複的資料切分。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, target_sampling_rate: int = 16000, test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"初始化 AudioDatasetProcessor_V3\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_single_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        \"\"\"\n",
    "        將單一 DataFrame 轉換為具有指定取樣率 Audio 特徵的 Dataset。\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 包含 'audio' 和 'sentence' 欄位的 DataFrame。\n",
    "\n",
    "        Returns:\n",
    "            Dataset: 已處理的 Dataset 物件。\n",
    "        \"\"\"\n",
    "        # 複製 'path' 欄位到 'audio'，此時 df 應包含 'path' 和 'sentence'\n",
    "        temp_df = df.copy()\n",
    "        temp_df['audio'] = temp_df['file']\n",
    "\n",
    "        dataset = Dataset.from_pandas(temp_df)\n",
    "        \n",
    "        # 轉換 audio 欄位型別並移除多餘的 path 欄位\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        執行完整的資料處理流程，包含欄位重新命名。\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: 包含 'train' 和 'test' splits 的最終物件。\n",
    "        \"\"\"\n",
    "        # 讀取 CSV\n",
    "        try:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"錯誤：找不到檔案 {self.file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # 關鍵步驟 1: 指定來源需要的欄位是 'path' 和 'transcription'\n",
    "        required_columns = ['transcription', 'file']\n",
    "        if not all(col in full_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV 檔案缺少必要欄位。需要: {required_columns}\")\n",
    "        \n",
    "        filtered_data = full_data[required_columns]\n",
    "\n",
    "        # 使用處理後的 DataFrame 進行資料切分\n",
    "        train_df, test_df = train_test_split(\n",
    "             filtered_data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # 分別為訓練集和測試集準備 Dataset\n",
    "        train_dataset = self._prepare_single_dataset(train_df)\n",
    "        test_dataset = self._prepare_single_dataset(test_df)\n",
    "\n",
    "        final_dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        })\n",
    "\n",
    "        return final_dataset_dict\n",
    "\n",
    "# --- 使用範例 ---\n",
    "if __name__ == '__main__':\n",
    "    # 假設您的 CSV 檔案名為 'final_audio_paths.csv'\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    \n",
    "    # 實例化最終版本的處理器\n",
    "    # 明確指定取樣率為 16000 Hz\n",
    "    processor_v3 = AudioDatasetProcessor_V3(file_path=CSV_PATH, target_sampling_rate=16000)\n",
    "    final_dataset = processor_v3.create_dataset_dict()\n",
    "\n",
    "    if final_dataset:\n",
    "        print(\"--- 最終版資料集結構 ---\")\n",
    "        print(final_dataset)\n",
    "        \n",
    "        print(\"\\n--- Train Split Features ---\")\n",
    "        print(final_dataset['train'].features)\n",
    "\n",
    "        print(\"\\n--- 抽查第一筆訓練資料 (驗證最終格式) ---\")\n",
    "        print(final_dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66819195",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "\n",
    "from transformers import WhisperTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "class AudioDatasetProcessor_V3:\n",
    "    \"\"\"\n",
    "    處理音訊 CSV 的最終版本類別。\n",
    "    - 讀取 'path' 與 'transcription' 欄位。\n",
    "    - 將 'transcription' 重新命名為 'sentence'。\n",
    "    - 產生與目標格式完全匹配的 DatasetDict。\n",
    "\n",
    "    屬性:\n",
    "        file_path (str): 輸入的 CSV 檔案路徑。\n",
    "        target_sampling_rate (int): 目標音訊取樣率。\n",
    "        test_size (float): 用於劃分測試集的資料比例。\n",
    "        random_state (int): 隨機種子，確保可重複的資料切分。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str, target_sampling_rate: int = 16000, test_size: float = 0.2, random_state: int = 42):\n",
    "        \"\"\"初始化 AudioDatasetProcessor_V3\"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _prepare_single_dataset(self, df: pd.DataFrame) -> Dataset:\n",
    "        \"\"\"\n",
    "        將單一 DataFrame 轉換為具有指定取樣率 Audio 特徵的 Dataset。\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): 包含 'audio' 和 'sentence' 欄位的 DataFrame。\n",
    "\n",
    "        Returns:\n",
    "            Dataset: 已處理的 Dataset 物件。\n",
    "        \"\"\"\n",
    "        # 複製 'path' 欄位到 'audio'，此時 df 應包含 'path' 和 'sentence'\n",
    "        temp_df = df.copy()\n",
    "        temp_df['audio'] = temp_df['file']\n",
    "\n",
    "        dataset = Dataset.from_pandas(temp_df)\n",
    "        \n",
    "        # 轉換 audio 欄位型別並移除多餘的 path 欄位\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        執行完整的資料處理流程，包含欄位重新命名。\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: 包含 'train' 和 'test' splits 的最終物件。\n",
    "        \"\"\"\n",
    "        # 讀取 CSV\n",
    "        try:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"錯誤：找不到檔案 {self.file_path}\")\n",
    "            return None\n",
    "            \n",
    "        # 關鍵步驟 1: 指定來源需要的欄位是 'path' 和 'transcription'\n",
    "        required_columns = ['transcription', 'file']\n",
    "        if not all(col in full_data.columns for col in required_columns):\n",
    "            raise ValueError(f\"CSV 檔案缺少必要欄位。需要: {required_columns}\")\n",
    "        \n",
    "        filtered_data = full_data[required_columns]\n",
    "\n",
    "        # 使用處理後的 DataFrame 進行資料切分\n",
    "        train_df, test_df = train_test_split(\n",
    "             filtered_data,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # 分別為訓練集和測試集準備 Dataset\n",
    "        train_dataset = self._prepare_single_dataset(train_df)\n",
    "        test_dataset = self._prepare_single_dataset(test_df)\n",
    "\n",
    "        final_dataset_dict = DatasetDict({\n",
    "            \"train\": train_dataset,\n",
    "            \"test\": test_dataset\n",
    "        })\n",
    "\n",
    "        return final_dataset_dict\n",
    "\n",
    "# --- 使用範例 ---\n",
    "if __name__ == '__main__':\n",
    "    # 假設您的 CSV 檔案名為 'final_audio_paths.csv'\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    \n",
    "    # 實例化最終版本的處理器\n",
    "    # 明確指定取樣率為 16000 Hz\n",
    "    processor_v3 = AudioDatasetProcessor_V3(file_path=CSV_PATH, target_sampling_rate=16000)\n",
    "    final_dataset = processor_v3.create_dataset_dict()\n",
    "\n",
    "    if final_dataset:\n",
    "        print(\"--- 最終版資料集結構 ---\")\n",
    "        print(final_dataset)\n",
    "        \n",
    "        print(\"\\n--- Train Split Features ---\")\n",
    "        print(final_dataset['train'].features)\n",
    "\n",
    "        print(\"\\n--- 抽查第一筆訓練資料 (驗證最終格式) ---\")\n",
    "        print(final_dataset['train'][0])\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"zh\", task=\"transcribe\")\n",
    "from datasets import Audio\n",
    "\n",
    "common_voice = final_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "input_str = final_dataset[\"train\"][0][\"transcription\"]\n",
    "labels = tokenizer(input_str).input_ids\n",
    "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "final_dataset = final_dataset.map(prepare_dataset, remove_columns=final_dataset.column_names[\"train\"], num_proc=4)\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None\n",
    "\n",
    "# print(f\"Input:                 {input_str}\")\n",
    "# print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "# print(f\"Decoded w/out special: {decoded_str}\")\n",
    "# print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b48cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafa7399599449fd96fa757c2289d767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6509c2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步驟 1/4: 載入 Processor 和模型 ---\n",
      "\n",
      "--- 步驟 2/4: 建立原始資料集並設定『即時轉換』---\n",
      "即時轉換已設定。\n",
      "\n",
      "--- 步驟 3/4: 建立訓練元件 (最終穩定運行版) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24512\\4225368149.py:153: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步驟 4/4: 開始模型微調訓練 (使用最終穩定高效模式) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 180\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m最終模型已儲存至：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m'\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# 確保您已在終端機使用 `huggingface-cli login` 登入\u001b[39;00m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# 執行前建議重新啟動您的電腦，確保系統處於乾淨狀態\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 166\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- 步驟 4/4: 開始模型微調訓練 (使用最終穩定高效模式) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# 不帶參數的 .train() 會自動處理斷點續練，是最穩健的做法。\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    167\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m*** 訓練完成 ***\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# --- 儲存最終模型 ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2197\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2194\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2195\u001b[39m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[32m   2196\u001b[39m     hf_hub_utils.disable_progress_bars()\n\u001b[32m-> \u001b[39m\u001b[32m2197\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2203\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2204\u001b[39m     hf_hub_utils.enable_progress_bars()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3797\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3795\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3797\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\accelerate\\accelerator.py:2549\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2548\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2549\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.has_lomo_optimizer:\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\audio_model\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 檔案：train_final.py\n",
    "# 描述：一個完整、高效、穩健的 Whisper 模型微調流程的最終版本。\n",
    "# 核心策略：\n",
    "# 1. 即時轉換 (.with_transform)：徹底解決記憶體不足與預處理過久的問題。\n",
    "# 2. 背景預取 (dataloader_num_workers)：解決 CPU 與 I/O 瓶頸，最大化 GPU 使用率。\n",
    "# 3. 全域定義 (Global Scope)：解決多核心處理時的 pickling 錯誤。\n",
    "# 4. 智慧續練 (Smart Resuming)：自動從上次的檢查點恢復訓練。\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Union\n",
    "from dataclasses import dataclass\n",
    "import evaluate\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "# --- Hugging Face 相關導入 ---\n",
    "from huggingface_hub import login\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "\n",
    "# ==============================================================================\n",
    "# 步驟 1: 將所有輔助類別與函式定義在「全域範圍」\n",
    "# 這是為了確保在使用 dataloader_num_workers > 0 時，背景程序可以成功序列化 (pickle) 它們。\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"處理語音到序列資料的 Data Collator，負責將樣本整理成批次並進行填充。\"\"\"\n",
    "    processor: Any\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset_batched(batch, feature_extractor, tokenizer):\n",
    "    \"\"\"將一批音訊和文本資料『即時』轉換為模型輸入格式。\"\"\"\n",
    "    audio_list = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        [x[\"array\"] for x in audio_list], \n",
    "        sampling_rate=audio_list[0][\"sampling_rate\"]\n",
    "    ).input_features\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"], max_length=448, truncation=True).input_ids\n",
    "    return batch\n",
    "\n",
    "def compute_metrics(pred, tokenizer):\n",
    "    \"\"\"在評估階段，計算並回傳 WER 指標。\"\"\"\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# ==============================================================================\n",
    "# 步驟 2: 主執行流程\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    # --- 參數設定 ---\n",
    "    CSV_PATH = 'output/final_audio_paths.csv'\n",
    "    MODEL_NAME = \"openai/whisper-small\"\n",
    "    LANGUAGE = \"zh\"\n",
    "    TASK = \"transcribe\"\n",
    "    OUTPUT_DIR = \"./whisper-small-zh-finetune-final\"\n",
    "\n",
    "    # --- 載入 Processor 和模型 ---\n",
    "    print(\"--- 步驟 1/4: 載入 Processor 和模型 ---\")\n",
    "    processor = WhisperProcessor.from_pretrained(MODEL_NAME, language=LANGUAGE, task=TASK)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # --- 建立原始資料集 ---\n",
    "    class AudioDatasetProcessor:\n",
    "        def __init__(self, file_path: str, target_sampling_rate: int = 16000):\n",
    "            self.file_path = file_path\n",
    "            self.target_sampling_rate = target_sampling_rate\n",
    "        def create_dataset(self) -> Dataset:\n",
    "            full_data = pd.read_csv(self.file_path)\n",
    "            dataset = Dataset.from_pandas(full_data)\n",
    "            dataset = dataset.cast_column(\"file\", Audio(sampling_rate=self.target_sampling_rate))\n",
    "            dataset = dataset.rename_column(\"file\", \"audio\")\n",
    "            return dataset\n",
    "            \n",
    "    print(\"\\n--- 步驟 2/4: 建立原始資料集並設定『即時轉換』---\")\n",
    "    audio_processor = AudioDatasetProcessor(file_path=CSV_PATH)\n",
    "    full_dataset = audio_processor.create_dataset()\n",
    "    common_voice = full_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    # 使用 .with_transform() 確保記憶體穩定，訓練能立刻開始\n",
    "    prepare_fn = partial(prepare_dataset_batched, feature_extractor=processor.feature_extractor, tokenizer=processor.tokenizer)\n",
    "    vectorized_datasets = common_voice.with_transform(prepare_fn)\n",
    "    print(\"即時轉換已設定。\")\n",
    "\n",
    "    # --- 建立訓練元件 ---\n",
    "    print(\"\\n--- 步驟 3/4: 建立訓練元件 (最終穩定運行版) ---\")\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)\n",
    "\n",
    "    # [最終修正]\n",
    "    # 既然 dataloader_num_workers=0 是唯一能避免死鎖的方式，\n",
    "    # 我們必須將訓練參數調整到與之匹配的保守水平，以避免 OOM 錯誤。\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        \n",
    "        # 1. 大幅降低批次大小，這是避免 OOM 的核心\n",
    "        per_device_train_batch_size=4,   # 從 32 或 16 大幅降至 4，這是一個極度安全的值\n",
    "        per_device_eval_batch_size=4,    # 驗證批次也使用同樣的安全值\n",
    "        \n",
    "        # 2. 適度使用梯度累積，以穩定訓練\n",
    "        # 有效批次大小為 4 * 4 = 16，這是一個不錯的平衡點\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        # 3. 禁用多核心處理，這是確保程式不被掛起的關鍵\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # --- 其他參數維持不變 ---\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=5000,\n",
    "        gradient_checkpointing=False,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=225,\n",
    "        save_steps=1000,\n",
    "        eval_steps=1000,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=True,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=vectorized_datasets[\"train\"],\n",
    "        eval_dataset=vectorized_datasets[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics_fn,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    # --- 開始訓練 ---\n",
    "    print(\"\\n--- 步驟 4/4: 開始模型微調訓練 (使用最終穩定高效模式) ---\")\n",
    "    # 不帶參數的 .train() 會自動處理斷點續練，是最穩健的做法。\n",
    "    trainer.train() \n",
    "    print(\"\\n*** 訓練完成 ***\")\n",
    "    \n",
    "    # --- 儲存最終模型 ---\n",
    "    print(\"\\n--- 正在儲存最終的最佳模型 ---\")\n",
    "    final_model_path = training_args.output_dir\n",
    "    trainer.save_model(final_model_path)\n",
    "    processor.save_pretrained(final_model_path)\n",
    "    print(f\"\\n最終模型已儲存至：{final_model_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 確保您已在終端機使用 `huggingface-cli login` 登入\n",
    "    # 執行前建議重新啟動您的電腦，確保系統處於乾淨狀態\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbcefe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PyTorch GPU 環境檢測報告 ---\n",
      "\n",
      "[✅ 成功] `torch.cuda.is_available()` 回報：True\n",
      "您的環境已準備好進行 GPU 加速訓練！\n",
      "\n",
      "[ℹ️ 資訊] 偵測到的可用 GPU 數量：1 張\n",
      "[ℹ️ 資訊] 目前 PyTorch 將使用的 GPU：NVIDIA GeForce RTX 3060 Ti (cuda:0)\n",
      "\n",
      "--- 檢測報告結束 ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    一個獨立的函式，用於檢查 PyTorch 的 GPU (CUDA) 環境是否設定正確。\n",
    "    \"\"\"\n",
    "    print(\"--- PyTorch GPU 環境檢測報告 ---\")\n",
    "    \n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    if is_cuda_available:\n",
    "        print(\"\\n[✅ 成功] `torch.cuda.is_available()` 回報：True\")\n",
    "        print(\"您的環境已準備好進行 GPU 加速訓練！\")\n",
    "        \n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n[ℹ️ 資訊] 偵測到的可用 GPU 數量：{gpu_count} 張\")\n",
    "        \n",
    "        current_device_id = torch.cuda.current_device()\n",
    "        current_device_name = torch.cuda.get_device_name(current_device_id)\n",
    "        print(f\"[ℹ️ 資訊] 目前 PyTorch 將使用的 GPU：{current_device_name} (cuda:{current_device_id})\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n[❌ 失敗] `torch.cuda.is_available()` 回報：False\")\n",
    "        print(\"您的環境目前無法使用 GPU，訓練將會由 CPU 執行，速度會非常慢。\")\n",
    "        \n",
    "    print(\"\\n--- 檢測報告結束 ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1693e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "c:\\Users\\User\\audio_model\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)\n",
    "import sys; print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"--- Python 系統路徑 (sys.path) 搜查報告 ---\")\n",
    "print(\"Python 會依照以下順序尋找函式庫：\")\n",
    "print(\"\\n\".join(sys.path))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "conflict_found = False\n",
    "# 遍歷所有 Python 會尋找的路徑\n",
    "for path in sys.path:\n",
    "    # 我們只關心真實存在的資料夾\n",
    "    if os.path.isdir(path):\n",
    "        # 尋找任何可能造成衝突的檔案\n",
    "        potential_conflict_file = os.path.join(path, \"datasets.py\")\n",
    "        if os.path.exists(potential_conflict_file):\n",
    "            print(f\"[🚨 重大發現!] 在以下路徑中找到了名為 datasets.py 的衝突檔案：\")\n",
    "            print(f\"==> {potential_conflict_file}\")\n",
    "            print(\"這就是造成您錯誤的根本原因！請立刻將此檔案重新命名或刪除。\")\n",
    "            conflict_found = True\n",
    "            break # 找到一個就夠了\n",
    "\n",
    "if not conflict_found:\n",
    "    print(\"[✅ 正常] 在所有 Python 搜尋路徑中，未發現名為 datasets.py 的衝突檔案。\")\n",
    "    print(\"這表示問題可能出在函式庫安裝本身。請繼續執行下一步。\")\n",
    "\n",
    "print(\"\\n--- 正在定位 `datasets` 函式庫的實際位置 ---\")\n",
    "try:\n",
    "    import datasets\n",
    "    # __file__ 屬性會告訴我們這個模組是從哪個檔案載入的\n",
    "    print(f\"[ℹ️ 資訊] 當您 `import datasets` 時，Python 實際載入的檔案是：\")\n",
    "    print(f\"==> {datasets.__file__}\")\n",
    "    print(\"請檢查這個路徑是否在您的 .venv 虛擬環境的 site-packages 中。如果不是，代表您的環境設定有誤。\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ 錯誤] 甚至無法成功 `import datasets`，這強烈暗示您的安裝已損壞。\")\n",
    "    print(f\"錯誤訊息: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0247ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "    \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-hi\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=5000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"\n",
    "    一個獨立的函式，用於檢查 PyTorch 的 GPU (CUDA) 環境是否設定正確。\n",
    "    \"\"\"\n",
    "    print(\"--- PyTorch GPU 環境檢測報告 ---\")\n",
    "    \n",
    "    # 1. 核心檢測：torch.cuda.is_available()\n",
    "    #    這是判斷 PyTorch 能否使用 GPU 的黃金標準。\n",
    "    #    它會檢查 NVIDIA 驅動、CUDA 工具包是否都已安裝且被 PyTorch 正確偵測。\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    \n",
    "    if is_cuda_available:\n",
    "        print(\"\\n[✅ 成功] `torch.cuda.is_available()` 回報：True\")\n",
    "        print(\"恭喜！您的 PyTorch 環境已成功偵測到並可以使用 GPU (CUDA)。\")\n",
    "        \n",
    "        # 2. 獲取 GPU 數量\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"\\n[ℹ️ 資訊] 偵測到的可用 GPU 數量：{gpu_count} 張\")\n",
    "        \n",
    "        # 3. 獲取當前使用的 GPU 資訊\n",
    "        #    PyTorch 預設使用第 0 張 GPU。\n",
    "        current_device_id = torch.cuda.current_device()\n",
    "        current_device_name = torch.cuda.get_device_name(current_device_id)\n",
    "        print(f\"[ℹ️ 資訊] 目前 PyTorch 預設使用的 GPU 編號：{current_device_id}\")\n",
    "        print(f\"[ℹ️ 資訊] GPU 型號：{current_device_name}\")\n",
    "        \n",
    "        # 4. 實戰驗證：嘗試將張量 (Tensor) 移至 GPU 並進行計算\n",
    "        print(\"\\n--- 正在進行實戰驗證 ---\")\n",
    "        try:\n",
    "            # 創建一個張量並明確指定要使用的 CUDA 設備\n",
    "            device = torch.device(\"cuda\")\n",
    "            tensor_cpu = torch.randn(3, 4)\n",
    "            print(f\"步驟 1: 在 CPU 上建立一個張量，位置：{tensor_cpu.device}\")\n",
    "            \n",
    "            # 使用 .to(device) 將張量移至 GPU\n",
    "            tensor_gpu = tensor_cpu.to(device)\n",
    "            print(f\"步驟 2: 已成功將張量複製到 GPU，新位置：{tensor_gpu.device}\")\n",
    "            \n",
    "            # 在 GPU 上執行一個簡單的矩陣乘法\n",
    "            result_gpu = torch.matmul(tensor_gpu.T, tensor_gpu)\n",
    "            print(\"步驟 3: 已在 GPU 上成功完成矩陣乘法運算。\")\n",
    "            \n",
    "            print(\"\\n[✅ 結論] GPU 驗證成功！您的環境已準備好進行高效的深度學習訓練。\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"\\n[❌ 錯誤] 在實戰驗證過程中發生錯誤，這不應該發生。請檢查您的驅動程式。\")\n",
    "            print(f\"錯誤訊息：{e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"\\n[❌ 失敗] `torch.cuda.is_available()` 回報：False\")\n",
    "        print(\"很抱歉，您的 PyTorch 環境目前無法使用 GPU。\")\n",
    "        print(\"可能的原因包括：\")\n",
    "        print(\"  1. 您的電腦沒有安裝 NVIDIA 的 GPU。\")\n",
    "        print(\"  2. 您尚未安裝 NVIDIA 官方驅動程式。\")\n",
    "        print(\"  3. 您安裝的 PyTorch 版本是 CPU-only 版本。\")\n",
    "        print(\"  4. CUDA 工具包與 PyTorch 或驅動程式版本不相容。\")\n",
    "        print(\"\\n建議操作：請確認以上幾點，並考慮在有 GPU 的環境（如 Google Colab）中執行您的訓練程式碼。\")\n",
    "        \n",
    "    print(\"\\n--- 檢測報告結束 ---\")\n",
    "\n",
    "# 執行檢測函式\n",
    "if __name__ == '__main__':\n",
    "    check_gpu_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f07652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import librosa\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer\n",
    "\n",
    "# 假設你的資料集是 CSV 格式\n",
    "data = pd.read_csv('output/final_audio_paths.csv')\n",
    "\n",
    "# 將資料轉換為 Hugging Face Dataset 格式\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data),\n",
    "    \"test\": Dataset.from_pandas(data.sample(frac=0.2))  # 隨機取 20% 作為測試集\n",
    "})\n",
    "\n",
    "# 定義 load_audio 函數\n",
    "def load_audio(file_path):\n",
    "    \"\"\"加載音頻文件並返回音頻數組和取樣率\"\"\"\n",
    "    audio_array, sampling_rate = librosa.load(file_path, sr=None)  # sr=None 保持原始取樣率\n",
    "    return audio_array, sampling_rate\n",
    "\n",
    "# 初始化 feature_extractor 和 tokenizer\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"zh\", task=\"transcribe\")\n",
    "\n",
    "# 定義數據集處理函數\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"file\"]  # 假設這是音頻文件的路徑\n",
    "    # 讀取音頻文件並轉換\n",
    "    audio_array, sampling_rate = load_audio(audio)  # 使用自定義的 load_audio 函數\n",
    "    batch[\"input_features\"] = feature_extractor(audio_array, sampling_rate=sampling_rate).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# 使用 map 方法處理數據集\n",
    "# dataset[\"train\"] = dataset[\"train\"].map(prepare_dataset, remove_columns=[\"transcription\", \"file\"], num_proc=4)\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"transcription\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=16,  # 減少批次大小\n",
    "    keep_in_memory=False,  # 設置為 False\n",
    "    num_proc=4  # 使用 4 個進程\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=[\"transcription\", \"file\"],\n",
    "    batched=True,\n",
    "    batch_size=16,  # 減少批次大小\n",
    "    keep_in_memory=False,  # 設置為 False\n",
    "    num_proc=4  # 使用 4 個進程\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
