#!/usr/bin/env python3
# ==============================================================================
# æª”æ¡ˆï¼šreset_gpu_memory.py
# æè¿°ï¼šé‡è¨­ GPU è¨˜æ†¶é«”ä¸¦æª¢æŸ¥ç³»çµ±ç‹€æ…‹
# ==============================================================================

import gc
import os
import subprocess
import sys

import torch


def force_memory_cleanup():
    """å¼·åˆ¶æ¸…ç†æ‰€æœ‰è¨˜æ†¶é«”"""
    print("ğŸ§¹ é–‹å§‹å¼·åˆ¶è¨˜æ†¶é«”æ¸…ç†...")

    # 1. Python åƒåœ¾å›æ”¶ï¼ˆå¤šæ¬¡åŸ·è¡Œï¼‰
    for i in range(5):
        collected = gc.collect()
        print(f"   åƒåœ¾å›æ”¶ç¬¬ {i+1} æ¬¡ï¼šæ¸…ç†äº† {collected} å€‹ç‰©ä»¶")

    # 2. CUDA è¨˜æ†¶é«”æ¸…ç†
    if torch.cuda.is_available():
        print("   æ¸…ç† CUDA è¨˜æ†¶é«”å¿«å–...")
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

        # æª¢æŸ¥è¨˜æ†¶é«”ç‹€æ…‹
        allocated = torch.cuda.memory_allocated() / (1024**3)
        reserved = torch.cuda.memory_reserved() / (1024**3)
        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)

        print(
            f"   è¨˜æ†¶é«”ç‹€æ…‹ï¼š{allocated:.2f}GB åˆ†é… / {reserved:.2f}GB ä¿ç•™ / {total:.2f}GB ç¸½è¨ˆ"
        )

        if allocated > 0.1:  # å¦‚æœé‚„æœ‰è¶…é 100MB è¢«åˆ†é…
            print("   âš ï¸ ä»æœ‰è¨˜æ†¶é«”è¢«ä½”ç”¨ï¼Œå»ºè­°é‡å•Ÿ Python ç¨‹åº")
        else:
            print("   âœ… CUDA è¨˜æ†¶é«”å·²æ¸…ç†")
    else:
        print("   âš ï¸ CUDA ä¸å¯ç”¨")


def check_gpu_processes():
    """æª¢æŸ¥ GPU é€²ç¨‹"""
    print("\nğŸ” æª¢æŸ¥ GPU é€²ç¨‹...")

    try:
        result = subprocess.run(
            [
                "nvidia-smi",
                "--query-compute-apps=pid,process_name,used_memory",
                "--format=csv,noheader,nounits",
            ],
            capture_output=True,
            text=True,
        )

        if result.returncode == 0:
            lines = result.stdout.strip().split("\n")
            if lines and lines[0]:
                print("   ç™¼ç¾ GPU é€²ç¨‹ï¼š")
                for line in lines:
                    if line.strip():
                        parts = line.split(", ")
                        if len(parts) >= 3:
                            pid, name, memory = parts[0], parts[1], parts[2]
                            print(f"     PID: {pid}, ç¨‹åº: {name}, è¨˜æ†¶é«”: {memory}MB")
            else:
                print("   âœ… æ²’æœ‰ GPU é€²ç¨‹")
        else:
            print("   âš ï¸ ç„¡æ³•åŸ·è¡Œ nvidia-smi")

    except FileNotFoundError:
        print("   âš ï¸ æ‰¾ä¸åˆ° nvidia-smi")
    except Exception as e:
        print(f"   âŒ æª¢æŸ¥å¤±æ•—ï¼š{e}")


def test_memory_allocation():
    """æ¸¬è©¦è¨˜æ†¶é«”åˆ†é…"""
    print("\nğŸ§ª æ¸¬è©¦è¨˜æ†¶é«”åˆ†é…...")

    if not torch.cuda.is_available():
        print("   è·³éï¼ˆCUDA ä¸å¯ç”¨ï¼‰")
        return False

    try:
        # æ¸¬è©¦åˆ†é… 100MB
        test_tensor = torch.rand(100 * 1024 * 1024 // 4, device="cuda")
        print("   âœ… èƒ½å¤ åˆ†é… 100MB")
        del test_tensor
        torch.cuda.empty_cache()
        return True
    except RuntimeError as e:
        print(f"   âŒ ç„¡æ³•åˆ†é…è¨˜æ†¶é«”ï¼š{e}")
        return False


def provide_troubleshooting():
    """æä¾›æ•…éšœæ’é™¤å»ºè­°"""
    print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè­°ï¼š")
    print("1. å¦‚æœè¨˜æ†¶é«”ä»è¢«ä½”ç”¨ï¼š")
    print("   - é‡å•Ÿ Jupyter Notebook/Python ç’°å¢ƒ")
    print("   - é‡å•Ÿçµ‚ç«¯æ©Ÿ")
    print("   - é‡å•Ÿé›»è…¦ï¼ˆæœ€å¾¹åº•ï¼‰")
    print()
    print("2. é‹è¡Œè¨“ç·´å‰ï¼š")
    print("   - é—œé–‰ç€è¦½å™¨å’Œå…¶ä»–ç¨‹å¼")
    print("   - é‹è¡Œé€™å€‹è…³æœ¬æ¸…ç†è¨˜æ†¶é«”")
    print("   - ä½¿ç”¨è¨˜æ†¶é«”å®‰å…¨ç‰ˆæœ¬ï¼šfinetune_Breeze_memory_safe.py")
    print()
    print("3. å¦‚æœå•é¡ŒæŒçºŒï¼š")
    print("   - é™ä½æ‰¹æ¬¡å¤§å°åˆ° 1")
    print("   - æ¸›å°‘è³‡æ–™é›†å¤§å°")
    print("   - è€ƒæ…®ä½¿ç”¨ CPU è¨“ç·´")


def main():
    print("=== GPU è¨˜æ†¶é«”é‡è¨­å·¥å…· ===\n")

    # åŸ·è¡Œæ¸…ç†
    force_memory_cleanup()

    # æª¢æŸ¥é€²ç¨‹
    check_gpu_processes()

    # æ¸¬è©¦åˆ†é…
    success = test_memory_allocation()

    # æä¾›å»ºè­°
    provide_troubleshooting()

    # ç¸½çµ
    print("\n=== ç¸½çµ ===")
    if success:
        print("âœ… GPU è¨˜æ†¶é«”å·²æ¸…ç†ï¼Œå¯ä»¥é–‹å§‹è¨“ç·´")
        print("å»ºè­°ä½¿ç”¨ï¼špython finetune_Breeze_memory_safe.py")
    else:
        print("âŒ GPU è¨˜æ†¶é«”å•é¡Œæœªè§£æ±º")
        print("å»ºè­°é‡å•Ÿ Python ç’°å¢ƒå¾Œå†è©¦")


if __name__ == "__main__":
    main()
