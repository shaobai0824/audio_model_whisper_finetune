# ==============================================================================
# æª”æ¡ˆï¼šfinetune_Breeze_ultra_minimal.py
# æè¿°ï¼šæ¥µåº¦ç²¾ç°¡ç‰ˆæœ¬ - å°ˆç‚º 8GB GPU è¨­è¨ˆ
# æ ¸å¿ƒç­–ç•¥ï¼š
# 1. æ¿€é€²çš„è¨˜æ†¶é«”ç®¡ç†
# 2. CPU å‚™é¸æ–¹æ¡ˆ
# 3. æœ€å°è³‡æ–™é›†å’Œæ¨¡å‹é…ç½®
# 4. è©³ç´°çš„è¨˜æ†¶é«”ç›£æ§
# ==============================================================================

import gc
import os
import sys
import warnings
from dataclasses import dataclass
from functools import partial
from typing import Any, Dict, List, Union

import pandas as pd
import psutil
import torch

# è¨­å®šç’°å¢ƒè®Šæ•¸ - æ¿€é€²çš„è¨˜æ†¶é«”ç®¡ç†
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True,max_split_size_mb:256"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from datasets import Audio, Dataset
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    WhisperFeatureExtractor,
    WhisperForConditionalGeneration,
    WhisperProcessor,
    WhisperTokenizer,
)

# ==============================================================================
# æ¿€é€²çš„è¨˜æ†¶é«”ç®¡ç†å·¥å…·
# ==============================================================================


def aggressive_cleanup():
    """æ¿€é€²çš„è¨˜æ†¶é«”æ¸…ç†"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        torch.cuda.ipc_collect()
        # å¼·åˆ¶å›æ”¶ CUDA è¨˜æ†¶é«”
        try:
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
        except:
            pass


def check_system_memory():
    """æª¢æŸ¥ç³»çµ±å’Œ GPU è¨˜æ†¶é«”"""
    # ç³»çµ±è¨˜æ†¶é«”
    memory = psutil.virtual_memory()
    print(
        f"ğŸ’¾ ç³»çµ±è¨˜æ†¶é«”ï¼š{memory.used/1024**3:.2f}GB ä½¿ç”¨ / {memory.total/1024**3:.2f}GB ç¸½è¨ˆ"
    )

    # GPU è¨˜æ†¶é«”
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / (1024**3)
            reserved = torch.cuda.memory_reserved(i) / (1024**3)
            total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(
                f"ğŸ”¥ GPU {i}ï¼š{allocated:.2f}GB åˆ†é… / {reserved:.2f}GB ä¿ç•™ / {total:.2f}GB ç¸½è¨ˆ"
            )

            # è­¦å‘Šè¨˜æ†¶é«”ä½¿ç”¨éé«˜
            if allocated > total * 0.8:
                print(f"âš ï¸  è­¦å‘Šï¼šGPU {i} è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ï¼")
                return False
    return True


def force_memory_reset():
    """å¼·åˆ¶é‡è¨­è¨˜æ†¶é«”ç‹€æ…‹"""
    print("ğŸ”„ å¼·åˆ¶é‡è¨­è¨˜æ†¶é«”...")
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        # å˜—è©¦é‡è¨­ CUDA ä¸Šä¸‹æ–‡
        try:
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
        except:
            pass
    gc.collect()


# ==============================================================================
# æ¥µç°¡è³‡æ–™è™•ç†å™¨
# ==============================================================================


@dataclass
class UltraMinimalDataCollator:
    """æ¥µç°¡ Data Collator"""

    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        try:
            # æ¥µç°¡è™•ç†
            input_features = [
                {"input_features": feature["input_features"]} for feature in features
            ]
            batch = self.processor.feature_extractor.pad(
                input_features, return_tensors="pt"
            )

            label_features = [{"input_ids": feature["labels"]} for feature in features]
            labels_batch = self.processor.tokenizer.pad(
                label_features, return_tensors="pt"
            )

            labels = labels_batch["input_ids"].masked_fill(
                labels_batch.attention_mask.ne(1), -100
            )

            if (
                (labels[:, 0] == self.processor.tokenizer.bos_token_id)
                .all()
                .cpu()
                .item()
            ):
                labels = labels[:, 1:]

            batch["labels"] = labels
            return batch
        except Exception as e:
            print(f"âŒ Data Collator éŒ¯èª¤ï¼š{e}")
            raise


def prepare_dataset_minimal(batch, feature_extractor, tokenizer):
    """æ¥µç°¡è³‡æ–™é è™•ç†"""
    try:
        audio_list = batch["audio"]

        # æ¥µç°¡éŸ³è¨Šè™•ç†
        input_features = feature_extractor(
            [x["array"] for x in audio_list],
            sampling_rate=audio_list[0]["sampling_rate"],
            return_tensors="np",
        ).input_features

        # æ¥µç°¡æ¨™ç±¤è™•ç†
        labels = tokenizer(
            batch["transcription"],
            max_length=224,  # æ¸›å°‘æœ€å¤§é•·åº¦
            truncation=True,
            return_tensors="np",
        ).input_ids

        return {"input_features": input_features, "labels": labels}
    except Exception as e:
        print(f"âŒ è³‡æ–™é è™•ç†éŒ¯èª¤ï¼š{e}")
        raise


# ==============================================================================
# æ¥µç°¡è³‡æ–™é›†è™•ç†å™¨
# ==============================================================================


class UltraMinimalDatasetProcessor:
    """æ¥µç°¡è³‡æ–™é›†è™•ç†å™¨ - åƒ…ä½¿ç”¨æœ€å°‘æ¨£æœ¬"""

    def __init__(self, file_path: str, target_sampling_rate: int = 16000):
        self.file_path = file_path
        self.target_sampling_rate = target_sampling_rate

    def create_dataset(self) -> Dataset:
        print(f"è¼‰å…¥è³‡æ–™æª”æ¡ˆï¼š{self.file_path}")

        try:
            df = pd.read_csv(self.file_path)
        except FileNotFoundError:
            print("âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼Œå˜—è©¦ä½¿ç”¨å‚™ç”¨è·¯å¾‘...")
            alternative_path = "output/final_audio_paths_zh.csv"
            df = pd.read_csv(alternative_path)
            print(f"âœ… ä½¿ç”¨å‚™ç”¨æª”æ¡ˆï¼š{alternative_path}")

        # ä½¿ç”¨æ¥µå°æ¨£æœ¬æ•¸ - åƒ… 10 å€‹æ¨£æœ¬
        subset_size = 10
        print(f"å®Œæ•´è³‡æ–™é›†å¤§å°ï¼š{len(df)}")
        print(f"ä½¿ç”¨è³‡æ–™é›†å¤§å°ï¼š{subset_size} (æ¥µå°æ¸¬è©¦é›†)")

        # å–å‰ 10 å€‹æ¨£æœ¬
        subset_data = df.head(subset_size).reset_index(drop=True)

        dataset = Dataset.from_pandas(subset_data)
        dataset = dataset.cast_column(
            "file", Audio(sampling_rate=self.target_sampling_rate)
        )
        dataset = dataset.rename_column("file", "audio")

        return dataset


# ==============================================================================
# ä¸»åŸ·è¡Œæµç¨‹ - æ”¯æ´ CPU å‚™é¸
# ==============================================================================


def main():
    print("=== Breeze ASR æ¥µåº¦ç²¾ç°¡ç‰ˆæœ¬ ===")
    print("ğŸ”§ å°ˆç‚º 8GB GPU è¨˜æ†¶é«”é™åˆ¶è¨­è¨ˆ")

    # --- åƒæ•¸è¨­å®š ---
    CSV_PATH = "output_zh_optimized_v2.csv"
    MODEL_NAME = "MediaTek-Research/Breeze-ASR-25"
    LANGUAGE = "zh"
    TASK = "transcribe"
    OUTPUT_DIR = "./whisper-small-zh-finetune-ultra-minimal"

    print(f"æ¨¡å‹ï¼š{MODEL_NAME}")
    print(f"è¼¸å‡ºç›®éŒ„ï¼š{OUTPUT_DIR}")

    # æª¢æŸ¥åˆå§‹è¨˜æ†¶é«”ç‹€æ…‹
    aggressive_cleanup()
    if not check_system_memory():
        print("âŒ åˆå§‹è¨˜æ†¶é«”ç‹€æ…‹ä¸ä½³ï¼Œæ­£åœ¨é‡è¨­...")
        force_memory_reset()

    # æ±ºå®šä½¿ç”¨ CPU é‚„æ˜¯ GPU
    use_cpu = False
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        if gpu_memory < 10:  # å¦‚æœ GPU è¨˜æ†¶é«”å°æ–¼ 10GBï¼Œè€ƒæ…®ä½¿ç”¨ CPU
            print(f"âš ï¸  GPU è¨˜æ†¶é«”ä¸è¶³ ({gpu_memory:.1f}GB)ï¼Œè€ƒæ…®ä½¿ç”¨ CPU æ¨¡å¼")
            use_cpu = True
    else:
        use_cpu = True

    device = "cpu" if use_cpu else "cuda"
    print(f"ğŸ”¥ ä½¿ç”¨è¨­å‚™ï¼š{device.upper()}")

    try:
        # --- è¼‰å…¥ Processor ---
        print("\n--- æ­¥é©Ÿ 1/4: è¼‰å…¥ Processor ---")
        processor = WhisperProcessor.from_pretrained(
            MODEL_NAME, language=LANGUAGE, task=TASK
        )
        print("âœ… Processor è¼‰å…¥æˆåŠŸ")

        # --- è¼‰å…¥æ¨¡å‹ ---
        print("\n--- æ­¥é©Ÿ 2/4: è¼‰å…¥æ¨¡å‹ ---")
        if use_cpu:
            print("ğŸ’¡ ä½¿ç”¨ CPU æ¨¡å¼é¿å… GPU è¨˜æ†¶é«”é™åˆ¶")
            model = WhisperForConditionalGeneration.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float32,
                device_map="cpu",
                low_cpu_mem_usage=True,
            )
        else:
            print("ğŸ’¡ ä½¿ç”¨æ¥µç°¡ GPU é…ç½®")
            model = WhisperForConditionalGeneration.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float16,  # ä½¿ç”¨ FP16 ç¯€çœè¨˜æ†¶é«”
                device_map="auto",
                low_cpu_mem_usage=True,
            )

        # é…ç½®æ¨¡å‹
        model.config.forced_decoder_ids = None
        model.config.suppress_tokens = []

        print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        check_system_memory()

        # --- å»ºç«‹æ¥µå°è³‡æ–™é›† ---
        print("\n--- æ­¥é©Ÿ 3/4: å»ºç«‹æ¥µå°è³‡æ–™é›† (10 æ¨£æœ¬) ---")
        audio_processor = UltraMinimalDatasetProcessor(file_path=CSV_PATH)
        dataset = audio_processor.create_dataset()
        print(f"è³‡æ–™é›†å»ºç«‹å®Œæˆï¼Œæ¨£æœ¬æ•¸ï¼š{len(dataset)}")

        # åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦é›†
        common_voice = dataset.train_test_split(test_size=0.2, seed=42)
        print(f"è¨“ç·´é›†ï¼š{len(common_voice['train'])} æ¨£æœ¬")
        print(f"æ¸¬è©¦é›†ï¼š{len(common_voice['test'])} æ¨£æœ¬")

        # è¨­å®šå³æ™‚è½‰æ›
        prepare_fn = partial(
            prepare_dataset_minimal,
            feature_extractor=processor.feature_extractor,
            tokenizer=processor.tokenizer,
        )
        vectorized_datasets = common_voice.with_transform(prepare_fn)
        print("å³æ™‚è½‰æ›å·²è¨­å®š")

        check_system_memory()

        # --- å»ºç«‹è¨“ç·´å…ƒä»¶ ---
        print("\n--- æ­¥é©Ÿ 4/4: å»ºç«‹æ¥µç°¡è¨“ç·´é…ç½® ---")
        data_collator = UltraMinimalDataCollator(processor=processor)

        # æ¥µç°¡è¨“ç·´åƒæ•¸
        training_args = Seq2SeqTrainingArguments(
            output_dir=OUTPUT_DIR,
            # æ¥µå°æ‰¹æ¬¡é…ç½®
            per_device_train_batch_size=1,
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=2,  # æ¸›å°‘ç´¯ç©æ­¥æ•¸
            # é—œé–‰å¤šé€²ç¨‹
            dataloader_num_workers=0,
            dataloader_pin_memory=False,
            # å­¸ç¿’åƒæ•¸
            learning_rate=1e-5,
            warmup_steps=2,
            max_steps=20,  # æ¥µå°‘æ­¥æ•¸
            # ç²¾åº¦è¨­å®š
            fp16=False if use_cpu else True,
            bf16=False,
            # ç°¡åŒ–çš„è©•ä¼°å’Œä¿å­˜
            eval_strategy="no",  # é—œé–‰è©•ä¼°ä»¥ç¯€çœè¨˜æ†¶é«”
            save_steps=20,
            logging_steps=5,
            # é—œé–‰ä¸å¿…è¦çš„åŠŸèƒ½
            report_to=[],
            load_best_model_at_end=False,
            save_total_limit=1,
            # å…¶ä»–è¨­å®š
            remove_unused_columns=False,
            optim="adamw_torch",
            gradient_checkpointing=True,  # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ç¯€çœè¨˜æ†¶é«”
        )

        # å»ºç«‹è¨“ç·´å™¨
        trainer = Seq2SeqTrainer(
            args=training_args,
            model=model,
            train_dataset=vectorized_datasets["train"],
            data_collator=data_collator,
            tokenizer=processor.feature_extractor,
        )

        check_system_memory()

        # --- é–‹å§‹è¨“ç·´ ---
        print("\n--- é–‹å§‹æ¥µç°¡è¨“ç·´ ---")
        print("ğŸš€ é æœŸè¨“ç·´æ™‚é–“ï¼š1-2 åˆ†é˜")
        print(f"ğŸ’¡ ä½¿ç”¨ {device.upper()} æ¨¡å¼")

        aggressive_cleanup()
        check_system_memory()

        # é–‹å§‹è¨“ç·´
        trainer.train()
        print("\nâœ… æ¥µç°¡è¨“ç·´å®Œæˆï¼")

        # --- å„²å­˜æ¨¡å‹ ---
        print("\n--- å„²å­˜æ¨¡å‹ ---")
        trainer.save_model(OUTPUT_DIR)
        processor.save_pretrained(OUTPUT_DIR)
        print(f"æ¨¡å‹å·²å„²å­˜è‡³ï¼š{OUTPUT_DIR}")

        # --- è¨“ç·´æ‘˜è¦ ---
        print("\n=== æ¥µç°¡è¨“ç·´æ‘˜è¦ ===")
        print(f"âœ… æˆåŠŸå®Œæˆè¨“ç·´ï¼")
        print(f"ä½¿ç”¨è¨­å‚™ï¼š{device.upper()}")
        print(f"ä½¿ç”¨è³‡æ–™é‡ï¼š10 æ¨£æœ¬ (æ¥µå°æ¸¬è©¦)")
        print(f"è¨“ç·´æ­¥æ•¸ï¼š20 æ­¥")
        print(f"ç‹€æ…‹ï¼šç„¡éŒ¯èª¤å®Œæˆ")

    except torch.cuda.OutOfMemoryError as e:
        print(f"\nâŒ GPU è¨˜æ†¶é«”ä¸è¶³ï¼š{e}")
        print("\nğŸ”§ å»ºè­°è§£æ±ºæ–¹æ¡ˆï¼š")
        print("1. é‡å•Ÿ Python ç’°å¢ƒæ¸…ç†è¨˜æ†¶é«”")
        print("2. é—œé–‰å…¶ä»–ä½¿ç”¨ GPU çš„ç¨‹å¼")
        print("3. ä½¿ç”¨ CPU æ¨¡å¼ï¼šè¨­å®š use_cpu=True")
        force_memory_reset()
        raise e

    except Exception as e:
        print(f"\nâŒ è¨“ç·´å¤±æ•—ï¼š{e}")
        print(f"éŒ¯èª¤é¡å‹ï¼š{type(e).__name__}")
        aggressive_cleanup()
        raise e

    finally:
        aggressive_cleanup()
        print("ğŸ§¹ æœ€çµ‚æ¸…ç†å®Œæˆ")


if __name__ == "__main__":
    main()
